Reviewer B:


* I just have a few minor comments regarding the text. Firstly, the authors claim that FIt-SNE is not a PyPI/conda package in table 1, and that is just not the case. There is a similar comment in the introduction at the bottom of page 2 that needs to be remedied in this regard. "However, FIt-SNE is difficult to install and distribute, and provides and API largely inconsistent with the Python data science ecosystem".  I also find it fairly the comment fairly vague in the context of being inconsistent with Python data-science ecosystem, as the functional approach with most of the parameters being set by default is also a commonly used API for software.

We would like to thank the reviewer for pointing out the availability of PyPI/conda packages for FIt-SNE and have updated the text and Table 1 to reflect this.

* Finally, this is one of the only works that switches between the Barnes-Hut approach to the FIt-SNE algorithm and the authors mention that they have a heuristic for doing so. This is an extremely useful feature as well, as it chooses the best of both worlds, and it would be wonderful if the authors could provide a brief description for the heuristic they use in their code. 

We have now included a sentence in Section 3 on page 8 that describes our heuristic.

 

Recommendation: Accept Submission

------------------------------------------------------



------------------------------------------------------
Reviewer C:


MAJOR COMMENTS

* The paper describes openTSNE version 0.6, but the current version is 0.7, and it seems there were recent changes affecting some of the text and the code snippets in the paper (learning rate / momentum). This needs to be updated throughout. More generally, if the paper goes on to be published in JOSS, would it make sense to release version 1.0 and describe it as such in the paper? If the library is ready for a peer-reviewed publication, then is it ready to be designated 1.0?

We agree with the comments and have released version 1.0 of the software. We have updated all the code snippets to reflect the recent changes in the openTSNE package and have updated all version numbers in the text. We have also re-generated the figures accordingly.

* There are no details given about preprocessing transcriptomic data (normalization, log-transformation, gene selection, PCA, etc.) As the paper uses RNAseq datasets for all examples, these details should be provided somewhere.

We have added the details of the single-cell RNA-seq preprocessing pipelines to the Appendix.

* page 7: "one can speed up the overall computation by choosing a uniform kernel" -- this sounds cryptic as the uniform kernel was not explained anywhere. It could make sense to elaborate on it somewhere: what exactly is it? how to use it?

We have reworded the paragraph so that the uniform affinity kernel is now briefly described.

* page 7: initialization paragraph only mentions PCA, would it make sense to mention that openTSNE also supports Laplacian eigenmaps as initialization?

Thank you for the suggestion, we now mention this option in the initialization paragraph on page 7, and have also added a reference to Laplacian eigenmaps in Section 2.4 in the paragraph on global initialization.

* It would be interesting to add UMAP to the runtime benchmark. UMAP is a very popular alternative to t-SNE, and is often thought to be much faster, which is not actually the case for openTNSE. Perhaps a separate benchmark figure could focus on openTSNE vs UMAP (depending on the sample size)? This may depend on the used affinity model, as uniform affinity with 15 neighbors should be much faster than the standard affinity model (with 90 neighbors) for large sample sizes. If so (?), it may be of interest to show both and discuss.


MINOR COMMENTS

* top of page 5: you talk about attractive forces but they have not been introduced yet. It may make sense to move the equation from secton 2.2 into 2.1 after Eq (5).

We would like to thank the reviewer for noticing this error, and we have now moved the equation from section 2.2 to the end of section 2.1, and updated the text accordingly.

* page 6: "Special care must be taken to tune the learning rate during optimization" -- the details are missing here. So what values are used in openTSNE?

We have expanded the paragraph to elaborate on the instability and what parameter values we use in openTSNE.

* page 13: it is unclear what embedding.prepare_partial() does here -- how are the new samples initialized, before you are running optimize()? Same for the prepare_partial call on page 17.

We agree that this is somewhat crpytic and have added a sentence below the code snippet describing what each of the function calls does.

* Figure 7 -- "Data points representing cells are colored according to cell types assigned in the original publication; see the legend from Fig. 5 to map colors to cell type." The last part ("see the legend from Fig. 5 to map colors to cell type") is unclear as Figure 5 shows different dataset. Does this study use the same colormap? This is confusing.

We agree that this sentence is confusing and have reworded it to be more clear.

* References: Boehm et al is published.

We have updated the citation.
