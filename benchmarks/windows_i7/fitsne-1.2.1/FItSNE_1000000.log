--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 1000000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 83333.333333, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000000 x 50 data matrix successfully. X[0,0] = -2.913953
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000000, K: 90, N*K = 90000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 893.56s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000147)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 143.46 seconds), cost 10.024660
Iteration 100 (50 iterations in 144.29 seconds), cost 8.666266
Iteration 150 (50 iterations in 143.48 seconds), cost 8.272649
Iteration 200 (50 iterations in 142.20 seconds), cost 8.133045
Iteration 250 (50 iterations in 142.83 seconds), cost 8.064425
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 143.05 seconds), cost 6.722288
Iteration 350 (50 iterations in 144.59 seconds), cost 6.147166
Iteration 400 (50 iterations in 149.47 seconds), cost 5.836080
Iteration 450 (50 iterations in 143.35 seconds), cost 5.637102
Iteration 500 (50 iterations in 146.30 seconds), cost 5.497107
Iteration 550 (50 iterations in 142.68 seconds), cost 5.392936
Iteration 600 (50 iterations in 143.81 seconds), cost 5.311935
Iteration 650 (50 iterations in 149.94 seconds), cost 5.246806
Iteration 700 (50 iterations in 152.94 seconds), cost 5.193119
Iteration 750 (50 iterations in 151.93 seconds), cost 5.147721
Iteration 800 (50 iterations in 152.69 seconds), cost 5.108864
Iteration 850 (50 iterations in 154.23 seconds), cost 5.075064
Iteration 900 (50 iterations in 165.75 seconds), cost 5.045995
Iteration 950 (50 iterations in 168.83 seconds), cost 5.021659
Iteration 1000 (50 iterations in 174.77 seconds), cost 5.000287
Wrote the 1000000 x 2 data matrix successfully.
Done.

FIt-SNE: 4065.6993634700775
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 1000000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 83333.333333, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000000 x 50 data matrix successfully. X[0,0] = -2.686156
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000000, K: 90, N*K = 90000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 867.347s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000147)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 138.15 seconds), cost 10.024883
Iteration 100 (50 iterations in 138.66 seconds), cost 8.683849
Iteration 150 (50 iterations in 139.57 seconds), cost 8.354389
Iteration 200 (50 iterations in 140.64 seconds), cost 8.204297
Iteration 250 (50 iterations in 141.30 seconds), cost 8.094984
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 143.64 seconds), cost 6.737588
Iteration 350 (50 iterations in 140.44 seconds), cost 6.156257
Iteration 400 (50 iterations in 140.07 seconds), cost 5.844676
Iteration 450 (50 iterations in 144.44 seconds), cost 5.645044
Iteration 500 (50 iterations in 146.85 seconds), cost 5.504538
Iteration 550 (50 iterations in 148.82 seconds), cost 5.399912
Iteration 600 (50 iterations in 148.25 seconds), cost 5.318449
Iteration 650 (50 iterations in 147.96 seconds), cost 5.253087
Iteration 700 (50 iterations in 144.29 seconds), cost 5.199099
Iteration 750 (50 iterations in 149.74 seconds), cost 5.153577
Iteration 800 (50 iterations in 153.31 seconds), cost 5.114555
Iteration 850 (50 iterations in 164.12 seconds), cost 5.081039
Iteration 900 (50 iterations in 169.03 seconds), cost 5.053546
Iteration 950 (50 iterations in 173.55 seconds), cost 5.029786
Iteration 1000 (50 iterations in 170.52 seconds), cost 5.008566
Wrote the 1000000 x 2 data matrix successfully.
Done.

FIt-SNE: 4020.8725531101227
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 1000000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 83333.333333, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000000 x 50 data matrix successfully. X[0,0] = -2.964171
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000000, K: 90, N*K = 90000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 876.859s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000147)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 136.60 seconds), cost 10.025618
Iteration 100 (50 iterations in 143.26 seconds), cost 8.679244
Iteration 150 (50 iterations in 146.45 seconds), cost 8.297104
Iteration 200 (50 iterations in 147.49 seconds), cost 8.164284
Iteration 250 (50 iterations in 144.26 seconds), cost 8.092464
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 145.12 seconds), cost 6.730798
Iteration 350 (50 iterations in 145.86 seconds), cost 6.153426
Iteration 400 (50 iterations in 143.79 seconds), cost 5.841081
Iteration 450 (50 iterations in 141.87 seconds), cost 5.641089
Iteration 500 (50 iterations in 143.05 seconds), cost 5.500987
Iteration 550 (50 iterations in 144.67 seconds), cost 5.396717
Iteration 600 (50 iterations in 144.90 seconds), cost 5.315708
Iteration 650 (50 iterations in 149.32 seconds), cost 5.250493
Iteration 700 (50 iterations in 151.75 seconds), cost 5.196893
Iteration 750 (50 iterations in 157.12 seconds), cost 5.151574
Iteration 800 (50 iterations in 162.53 seconds), cost 5.112748
Iteration 850 (50 iterations in 162.16 seconds), cost 5.078957
Iteration 900 (50 iterations in 164.22 seconds), cost 5.050717
Iteration 950 (50 iterations in 172.12 seconds), cost 5.026895
Iteration 1000 (50 iterations in 170.81 seconds), cost 5.005843
Wrote the 1000000 x 2 data matrix successfully.
Done.

FIt-SNE: 4063.2235469818115
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 1000000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 83333.333333, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000000 x 50 data matrix successfully. X[0,0] = -2.429780
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000000, K: 90, N*K = 90000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 890.358s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000147)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 142.43 seconds), cost 10.024609
Iteration 100 (50 iterations in 142.93 seconds), cost 8.788103
Iteration 150 (50 iterations in 141.75 seconds), cost 8.412372
Iteration 200 (50 iterations in 141.47 seconds), cost 8.258936
Iteration 250 (50 iterations in 143.79 seconds), cost 8.151342
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 144.21 seconds), cost 6.748624
Iteration 350 (50 iterations in 145.82 seconds), cost 6.166341
Iteration 400 (50 iterations in 143.15 seconds), cost 5.848499
Iteration 450 (50 iterations in 142.58 seconds), cost 5.646330
Iteration 500 (50 iterations in 141.73 seconds), cost 5.505368
Iteration 550 (50 iterations in 146.69 seconds), cost 5.400646
Iteration 600 (50 iterations in 147.27 seconds), cost 5.319381
Iteration 650 (50 iterations in 151.41 seconds), cost 5.254006
Iteration 700 (50 iterations in 153.80 seconds), cost 5.200122
Iteration 750 (50 iterations in 153.63 seconds), cost 5.154662
Iteration 800 (50 iterations in 153.95 seconds), cost 5.115893
Iteration 850 (50 iterations in 154.99 seconds), cost 5.082089
Iteration 900 (50 iterations in 167.28 seconds), cost 5.053251
Iteration 950 (50 iterations in 174.16 seconds), cost 5.029190
Iteration 1000 (50 iterations in 173.71 seconds), cost 5.008022
Wrote the 1000000 x 2 data matrix successfully.
Done.

FIt-SNE: 4068.6053047180176
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 1000000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 83333.333333, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000000 x 50 data matrix successfully. X[0,0] = -2.593250
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000000, K: 90, N*K = 90000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 868.285s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000147)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 139.45 seconds), cost 10.025745
Iteration 100 (50 iterations in 138.75 seconds), cost 8.649585
Iteration 150 (50 iterations in 141.52 seconds), cost 8.290222
Iteration 200 (50 iterations in 142.10 seconds), cost 8.154610
Iteration 250 (50 iterations in 140.64 seconds), cost 8.085794
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 142.53 seconds), cost 6.733402
Iteration 350 (50 iterations in 140.84 seconds), cost 6.155896
Iteration 400 (50 iterations in 141.62 seconds), cost 5.842040
Iteration 450 (50 iterations in 144.13 seconds), cost 5.641028
Iteration 500 (50 iterations in 146.15 seconds), cost 5.500712
Iteration 550 (50 iterations in 145.74 seconds), cost 5.396319
Iteration 600 (50 iterations in 145.71 seconds), cost 5.315287
Iteration 650 (50 iterations in 144.11 seconds), cost 5.250271
Iteration 700 (50 iterations in 150.10 seconds), cost 5.196627
Iteration 750 (50 iterations in 152.15 seconds), cost 5.151411
Iteration 800 (50 iterations in 151.40 seconds), cost 5.112651
Iteration 850 (50 iterations in 154.65 seconds), cost 5.078971
Iteration 900 (50 iterations in 165.33 seconds), cost 5.050764
Iteration 950 (50 iterations in 169.18 seconds), cost 5.026916
Iteration 1000 (50 iterations in 172.65 seconds), cost 5.005830
Wrote the 1000000 x 2 data matrix successfully.
Done.

FIt-SNE: 4005.6547446250916
