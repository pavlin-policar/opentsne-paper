--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 100000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 8333.333333, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 100000 x 50 data matrix successfully. X[0,0] = -0.981272
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 100000, K: 90, N*K = 9000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 53.449s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.001414)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 5.84 seconds), cost 7.744720
Iteration 100 (50 iterations in 5.90 seconds), cost 6.452785
Iteration 150 (50 iterations in 5.91 seconds), cost 6.179865
Iteration 200 (50 iterations in 5.90 seconds), cost 6.088726
Iteration 250 (50 iterations in 5.92 seconds), cost 6.044796
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 6.09 seconds), cost 4.639368
Iteration 350 (50 iterations in 6.09 seconds), cost 4.131580
Iteration 400 (50 iterations in 6.32 seconds), cost 3.869208
Iteration 450 (50 iterations in 6.96 seconds), cost 3.704350
Iteration 500 (50 iterations in 7.71 seconds), cost 3.590246
Iteration 550 (50 iterations in 8.27 seconds), cost 3.506205
Iteration 600 (50 iterations in 8.79 seconds), cost 3.441693
Iteration 650 (50 iterations in 9.60 seconds), cost 3.390754
Iteration 700 (50 iterations in 10.43 seconds), cost 3.349261
Iteration 750 (50 iterations in 11.82 seconds), cost 3.314768
Iteration 800 (50 iterations in 12.24 seconds), cost 3.285458
Iteration 850 (50 iterations in 12.63 seconds), cost 3.260023
Iteration 900 (50 iterations in 14.68 seconds), cost 3.238622
Iteration 950 (50 iterations in 14.63 seconds), cost 3.219578
Iteration 1000 (50 iterations in 20.18 seconds), cost 3.202995
Wrote the 100000 x 2 data matrix successfully.
Done.

FIt-SNE: 251.76061296463013
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 100000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 8333.333333, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 100000 x 50 data matrix successfully. X[0,0] = -3.484652
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 100000, K: 90, N*K = 9000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 53.647s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.001414)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 6.09 seconds), cost 7.749818
Iteration 100 (50 iterations in 6.02 seconds), cost 6.467842
Iteration 150 (50 iterations in 5.95 seconds), cost 6.202749
Iteration 200 (50 iterations in 5.92 seconds), cost 6.109818
Iteration 250 (50 iterations in 5.90 seconds), cost 6.064798
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 6.07 seconds), cost 4.660045
Iteration 350 (50 iterations in 6.02 seconds), cost 4.151649
Iteration 400 (50 iterations in 6.45 seconds), cost 3.889868
Iteration 450 (50 iterations in 7.38 seconds), cost 3.725190
Iteration 500 (50 iterations in 7.62 seconds), cost 3.611217
Iteration 550 (50 iterations in 8.56 seconds), cost 3.527220
Iteration 600 (50 iterations in 9.25 seconds), cost 3.462693
Iteration 650 (50 iterations in 10.34 seconds), cost 3.411776
Iteration 700 (50 iterations in 11.56 seconds), cost 3.370375
Iteration 750 (50 iterations in 12.24 seconds), cost 3.335822
Iteration 800 (50 iterations in 13.02 seconds), cost 3.306322
Iteration 850 (50 iterations in 14.60 seconds), cost 3.281523
Iteration 900 (50 iterations in 14.58 seconds), cost 3.259676
Iteration 950 (50 iterations in 24.85 seconds), cost 3.242738
Iteration 1000 (50 iterations in 28.32 seconds), cost 3.233224
Wrote the 100000 x 2 data matrix successfully.
Done.

FIt-SNE: 276.417804479599
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 100000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 8333.333333, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 100000 x 50 data matrix successfully. X[0,0] = -2.121203
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 100000, K: 90, N*K = 9000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 53.923s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.001415)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 5.89 seconds), cost 7.756499
Iteration 100 (50 iterations in 5.85 seconds), cost 6.415750
Iteration 150 (50 iterations in 5.96 seconds), cost 6.183524
Iteration 200 (50 iterations in 5.99 seconds), cost 6.103501
Iteration 250 (50 iterations in 6.12 seconds), cost 6.059978
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 6.03 seconds), cost 4.648975
Iteration 350 (50 iterations in 6.04 seconds), cost 4.137309
Iteration 400 (50 iterations in 6.36 seconds), cost 3.874305
Iteration 450 (50 iterations in 7.14 seconds), cost 3.709541
Iteration 500 (50 iterations in 7.49 seconds), cost 3.595553
Iteration 550 (50 iterations in 8.26 seconds), cost 3.511553
Iteration 600 (50 iterations in 8.83 seconds), cost 3.447140
Iteration 650 (50 iterations in 9.70 seconds), cost 3.396005
Iteration 700 (50 iterations in 10.52 seconds), cost 3.354609
Iteration 750 (50 iterations in 12.04 seconds), cost 3.319947
Iteration 800 (50 iterations in 12.10 seconds), cost 3.290473
Iteration 850 (50 iterations in 13.24 seconds), cost 3.265081
Iteration 900 (50 iterations in 14.31 seconds), cost 3.243341
Iteration 950 (50 iterations in 14.34 seconds), cost 3.224098
Iteration 1000 (50 iterations in 22.64 seconds), cost 3.208569
Wrote the 100000 x 2 data matrix successfully.
Done.

FIt-SNE: 254.67154383659363
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 100000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 8333.333333, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 100000 x 50 data matrix successfully. X[0,0] = -1.816125
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 100000, K: 90, N*K = 9000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 53.255s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.001414)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 5.75 seconds), cost 7.756337
Iteration 100 (50 iterations in 5.80 seconds), cost 6.479810
Iteration 150 (50 iterations in 5.85 seconds), cost 6.239122
Iteration 200 (50 iterations in 5.88 seconds), cost 6.130907
Iteration 250 (50 iterations in 5.86 seconds), cost 6.069449
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 5.94 seconds), cost 4.652433
Iteration 350 (50 iterations in 5.94 seconds), cost 4.148118
Iteration 400 (50 iterations in 6.32 seconds), cost 3.886523
Iteration 450 (50 iterations in 6.99 seconds), cost 3.722234
Iteration 500 (50 iterations in 7.53 seconds), cost 3.608627
Iteration 550 (50 iterations in 8.22 seconds), cost 3.524891
Iteration 600 (50 iterations in 9.13 seconds), cost 3.460851
Iteration 650 (50 iterations in 9.71 seconds), cost 3.410317
Iteration 700 (50 iterations in 10.62 seconds), cost 3.369504
Iteration 750 (50 iterations in 12.36 seconds), cost 3.335235
Iteration 800 (50 iterations in 12.13 seconds), cost 3.305986
Iteration 850 (50 iterations in 13.45 seconds), cost 3.280788
Iteration 900 (50 iterations in 14.36 seconds), cost 3.259041
Iteration 950 (50 iterations in 14.38 seconds), cost 3.239836
Iteration 1000 (50 iterations in 25.20 seconds), cost 3.224530
Wrote the 100000 x 2 data matrix successfully.
Done.

FIt-SNE: 256.3918414115906
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 100000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 8333.333333, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 100000 x 50 data matrix successfully. X[0,0] = -3.796795
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 100000, K: 90, N*K = 9000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 53.63s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.001415)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 5.80 seconds), cost 7.747935
Iteration 100 (50 iterations in 5.93 seconds), cost 6.510974
Iteration 150 (50 iterations in 5.95 seconds), cost 6.245671
Iteration 200 (50 iterations in 6.01 seconds), cost 6.145943
Iteration 250 (50 iterations in 5.95 seconds), cost 6.096541
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 5.98 seconds), cost 4.655800
Iteration 350 (50 iterations in 6.11 seconds), cost 4.143639
Iteration 400 (50 iterations in 6.46 seconds), cost 3.880095
Iteration 450 (50 iterations in 7.44 seconds), cost 3.716046
Iteration 500 (50 iterations in 7.68 seconds), cost 3.602533
Iteration 550 (50 iterations in 8.60 seconds), cost 3.519078
Iteration 600 (50 iterations in 9.29 seconds), cost 3.454918
Iteration 650 (50 iterations in 10.29 seconds), cost 3.404208
Iteration 700 (50 iterations in 11.64 seconds), cost 3.362796
Iteration 750 (50 iterations in 12.36 seconds), cost 3.328166
Iteration 800 (50 iterations in 13.19 seconds), cost 3.298634
Iteration 850 (50 iterations in 14.66 seconds), cost 3.273820
Iteration 900 (50 iterations in 14.61 seconds), cost 3.252009
Iteration 950 (50 iterations in 25.08 seconds), cost 3.234818
Iteration 1000 (50 iterations in 24.64 seconds), cost 3.224773
Wrote the 100000 x 2 data matrix successfully.
Done.

FIt-SNE: 273.16391587257385
