--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
         n 1000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 200.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.191648
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (8 threads):
[===========================================================>] 99% 0.056s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134410)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.57 seconds), cost 3.227688
Iteration 100 (50 iterations in 0.58 seconds), cost 2.773580
Iteration 150 (50 iterations in 0.57 seconds), cost 2.771565
Iteration 200 (50 iterations in 0.56 seconds), cost 2.753940
Iteration 250 (50 iterations in 0.58 seconds), cost 2.770647
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.57 seconds), cost 1.038095
Iteration 350 (50 iterations in 0.59 seconds), cost 0.881901
Iteration 400 (50 iterations in 0.57 seconds), cost 0.833132
Iteration 450 (50 iterations in 0.70 seconds), cost 0.817804
Iteration 500 (50 iterations in 0.80 seconds), cost 0.811722
Iteration 550 (50 iterations in 0.93 seconds), cost 0.805606
Iteration 600 (50 iterations in 0.92 seconds), cost 0.808109
Iteration 650 (50 iterations in 0.97 seconds), cost 0.800037
Iteration 700 (50 iterations in 1.05 seconds), cost 0.804465
Iteration 750 (50 iterations in 1.03 seconds), cost 0.799594
Iteration 800 (50 iterations in 1.02 seconds), cost 0.793629
Iteration 850 (50 iterations in 1.10 seconds), cost 0.792140
Iteration 900 (50 iterations in 1.19 seconds), cost 0.788964
Iteration 950 (50 iterations in 1.22 seconds), cost 0.789065
Iteration 1000 (50 iterations in 1.20 seconds), cost 0.791530
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 16.915809154510498
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
         n 1000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 200.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.253909
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (8 threads):
[============================================================] 100% 0.051s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134254)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.56 seconds), cost 3.157863
Iteration 100 (50 iterations in 0.55 seconds), cost 2.780542
Iteration 150 (50 iterations in 0.57 seconds), cost 2.761370
Iteration 200 (50 iterations in 0.54 seconds), cost 2.768122
Iteration 250 (50 iterations in 0.55 seconds), cost 2.793019
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.55 seconds), cost 1.037846
Iteration 350 (50 iterations in 0.56 seconds), cost 0.885910
Iteration 400 (50 iterations in 0.61 seconds), cost 0.846878
Iteration 450 (50 iterations in 0.84 seconds), cost 0.829568
Iteration 500 (50 iterations in 0.96 seconds), cost 0.821849
Iteration 550 (50 iterations in 1.06 seconds), cost 0.816089
Iteration 600 (50 iterations in 1.08 seconds), cost 0.811975
Iteration 650 (50 iterations in 1.18 seconds), cost 0.806916
Iteration 700 (50 iterations in 1.24 seconds), cost 0.812143
Iteration 750 (50 iterations in 1.24 seconds), cost 0.804506
Iteration 800 (50 iterations in 1.19 seconds), cost 0.802819
Iteration 850 (50 iterations in 1.22 seconds), cost 0.800075
Iteration 900 (50 iterations in 1.24 seconds), cost 0.799773
Iteration 950 (50 iterations in 1.24 seconds), cost 0.800816
Iteration 1000 (50 iterations in 1.28 seconds), cost 0.794596
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 18.43363881111145
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
         n 1000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 200.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -2.336779
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (8 threads):
[===========================================================>] 99% 0.052s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.136232)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.56 seconds), cost 3.213439
Iteration 100 (50 iterations in 0.56 seconds), cost 2.835665
Iteration 150 (50 iterations in 0.60 seconds), cost 2.807347
Iteration 200 (50 iterations in 0.57 seconds), cost 2.821656
Iteration 250 (50 iterations in 0.56 seconds), cost 2.811283
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.55 seconds), cost 1.094642
Iteration 350 (50 iterations in 0.54 seconds), cost 0.946475
Iteration 400 (50 iterations in 0.58 seconds), cost 0.890056
Iteration 450 (50 iterations in 0.83 seconds), cost 0.876254
Iteration 500 (50 iterations in 0.99 seconds), cost 0.876797
Iteration 550 (50 iterations in 1.02 seconds), cost 0.871345
Iteration 600 (50 iterations in 1.15 seconds), cost 0.863200
Iteration 650 (50 iterations in 1.15 seconds), cost 0.860047
Iteration 700 (50 iterations in 1.14 seconds), cost 0.870639
Iteration 750 (50 iterations in 1.18 seconds), cost 0.862506
Iteration 800 (50 iterations in 1.26 seconds), cost 0.868385
Iteration 850 (50 iterations in 1.29 seconds), cost 0.866048
Iteration 900 (50 iterations in 1.32 seconds), cost 0.858033
Iteration 950 (50 iterations in 1.27 seconds), cost 0.859001
Iteration 1000 (50 iterations in 1.34 seconds), cost 0.848858
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 18.6662380695343
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
         n 1000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 200.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = 6.393470
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (8 threads):
[============================================================] 100% 0.055s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.133712)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.56 seconds), cost 3.225059
Iteration 100 (50 iterations in 0.55 seconds), cost 2.763040
Iteration 150 (50 iterations in 0.56 seconds), cost 2.744819
Iteration 200 (50 iterations in 0.57 seconds), cost 2.767218
Iteration 250 (50 iterations in 0.55 seconds), cost 2.724552
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.57 seconds), cost 1.027745
Iteration 350 (50 iterations in 0.54 seconds), cost 0.871340
Iteration 400 (50 iterations in 0.56 seconds), cost 0.836568
Iteration 450 (50 iterations in 0.70 seconds), cost 0.818248
Iteration 500 (50 iterations in 0.84 seconds), cost 0.808658
Iteration 550 (50 iterations in 0.90 seconds), cost 0.809733
Iteration 600 (50 iterations in 0.99 seconds), cost 0.802267
Iteration 650 (50 iterations in 1.01 seconds), cost 0.801306
Iteration 700 (50 iterations in 1.00 seconds), cost 0.797326
Iteration 750 (50 iterations in 1.15 seconds), cost 0.800327
Iteration 800 (50 iterations in 1.15 seconds), cost 0.798672
Iteration 850 (50 iterations in 1.16 seconds), cost 0.799632
Iteration 900 (50 iterations in 1.14 seconds), cost 0.796309
Iteration 950 (50 iterations in 1.18 seconds), cost 0.785054
Iteration 1000 (50 iterations in 1.25 seconds), cost 0.795220
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 17.07750105857849
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
         n 1000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 200.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -1.059645
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (8 threads):
[============================================================] 100% 0.053s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134126)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.56 seconds), cost 3.218593
Iteration 100 (50 iterations in 0.58 seconds), cost 2.796110
Iteration 150 (50 iterations in 0.57 seconds), cost 2.743307
Iteration 200 (50 iterations in 0.57 seconds), cost 2.705483
Iteration 250 (50 iterations in 0.56 seconds), cost 2.768551
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.54 seconds), cost 1.049135
Iteration 350 (50 iterations in 0.54 seconds), cost 0.882680
Iteration 400 (50 iterations in 0.62 seconds), cost 0.837155
Iteration 450 (50 iterations in 0.85 seconds), cost 0.824632
Iteration 500 (50 iterations in 0.97 seconds), cost 0.816112
Iteration 550 (50 iterations in 1.04 seconds), cost 0.806163
Iteration 600 (50 iterations in 1.15 seconds), cost 0.807838
Iteration 650 (50 iterations in 1.14 seconds), cost 0.805242
Iteration 700 (50 iterations in 1.24 seconds), cost 0.799253
Iteration 750 (50 iterations in 1.26 seconds), cost 0.798359
Iteration 800 (50 iterations in 1.24 seconds), cost 0.800425
Iteration 850 (50 iterations in 1.25 seconds), cost 0.798108
Iteration 900 (50 iterations in 1.34 seconds), cost 0.796964
Iteration 950 (50 iterations in 1.32 seconds), cost 0.795534
Iteration 1000 (50 iterations in 1.32 seconds), cost 0.796971
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 18.839783668518066
