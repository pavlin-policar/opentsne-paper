--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
         n 750000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 62500.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 750000 x 50 data matrix successfully. X[0,0] = 6.141911
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 750000, K: 90, N*K = 67500000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (8 threads):
[===========================================================>] 99% 119.766s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000195)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 36.26 seconds), cost 9.644606
Iteration 100 (50 iterations in 36.44 seconds), cost 8.278833
Iteration 150 (50 iterations in 36.56 seconds), cost 7.935044
Iteration 200 (50 iterations in 36.53 seconds), cost 7.815148
Iteration 250 (50 iterations in 36.50 seconds), cost 7.752113
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 36.70 seconds), cost 6.396549
Iteration 350 (50 iterations in 36.97 seconds), cost 5.827374
Iteration 400 (50 iterations in 38.19 seconds), cost 5.528109
Iteration 450 (50 iterations in 38.55 seconds), cost 5.335778
Iteration 500 (50 iterations in 39.77 seconds), cost 5.197873
Iteration 550 (50 iterations in 40.16 seconds), cost 5.099093
Iteration 600 (50 iterations in 41.21 seconds), cost 5.022627
Iteration 650 (50 iterations in 42.29 seconds), cost 4.959343
Iteration 700 (50 iterations in 44.24 seconds), cost 4.906778
Iteration 750 (50 iterations in 45.88 seconds), cost 4.867416
Iteration 800 (50 iterations in 47.75 seconds), cost 4.830412
Iteration 850 (50 iterations in 47.24 seconds), cost 4.794164
Iteration 900 (50 iterations in 56.82 seconds), cost 4.765045
Iteration 950 (50 iterations in 59.79 seconds), cost 4.741965
Iteration 1000 (50 iterations in 64.06 seconds), cost 4.722245
Wrote the 750000 x 2 data matrix successfully.
Done.

FIt-SNE: 1104.4874901771545
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
         n 750000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 62500.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 750000 x 50 data matrix successfully. X[0,0] = -1.970973
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 750000, K: 90, N*K = 67500000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (8 threads):
[===========================================================>] 99% 119.071s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000195)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 36.36 seconds), cost 9.643655
Iteration 100 (50 iterations in 36.59 seconds), cost 8.278693
Iteration 150 (50 iterations in 37.58 seconds), cost 7.967641
Iteration 200 (50 iterations in 36.94 seconds), cost 7.849898
Iteration 250 (50 iterations in 37.07 seconds), cost 7.785293
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 36.94 seconds), cost 6.434283
Iteration 350 (50 iterations in 36.81 seconds), cost 5.863742
Iteration 400 (50 iterations in 37.66 seconds), cost 5.555241
Iteration 450 (50 iterations in 38.65 seconds), cost 5.359147
Iteration 500 (50 iterations in 39.19 seconds), cost 5.220867
Iteration 550 (50 iterations in 40.79 seconds), cost 5.117068
Iteration 600 (50 iterations in 41.13 seconds), cost 5.038277
Iteration 650 (50 iterations in 43.07 seconds), cost 4.973316
Iteration 700 (50 iterations in 45.33 seconds), cost 4.922289
Iteration 750 (50 iterations in 45.88 seconds), cost 4.876967
Iteration 800 (50 iterations in 47.87 seconds), cost 4.837534
Iteration 850 (50 iterations in 47.88 seconds), cost 4.805292
Iteration 900 (50 iterations in 59.05 seconds), cost 4.778493
Iteration 950 (50 iterations in 64.32 seconds), cost 4.754924
Iteration 1000 (50 iterations in 64.98 seconds), cost 4.735539
Wrote the 750000 x 2 data matrix successfully.
Done.

FIt-SNE: 1116.5397074222565
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
         n 750000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 62500.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 750000 x 50 data matrix successfully. X[0,0] = -1.893229
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 750000, K: 90, N*K = 67500000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (8 threads):
[===========================================================>] 99% 121.797s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000195)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 36.42 seconds), cost 9.646909
Iteration 100 (50 iterations in 36.41 seconds), cost 8.279479
Iteration 150 (50 iterations in 37.04 seconds), cost 7.948379
Iteration 200 (50 iterations in 36.69 seconds), cost 7.825370
Iteration 250 (50 iterations in 37.42 seconds), cost 7.756307
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 37.17 seconds), cost 6.415506
Iteration 350 (50 iterations in 37.20 seconds), cost 5.848526
Iteration 400 (50 iterations in 38.10 seconds), cost 5.547126
Iteration 450 (50 iterations in 39.21 seconds), cost 5.353833
Iteration 500 (50 iterations in 39.94 seconds), cost 5.222595
Iteration 550 (50 iterations in 40.50 seconds), cost 5.119257
Iteration 600 (50 iterations in 41.69 seconds), cost 5.041421
Iteration 650 (50 iterations in 44.37 seconds), cost 4.977520
Iteration 700 (50 iterations in 44.64 seconds), cost 4.926694
Iteration 750 (50 iterations in 46.45 seconds), cost 4.883801
Iteration 800 (50 iterations in 47.49 seconds), cost 4.845745
Iteration 850 (50 iterations in 54.94 seconds), cost 4.813150
Iteration 900 (50 iterations in 59.61 seconds), cost 4.787174
Iteration 950 (50 iterations in 64.70 seconds), cost 4.764632
Iteration 1000 (50 iterations in 63.75 seconds), cost 4.744262
Wrote the 750000 x 2 data matrix successfully.
Done.

FIt-SNE: 1130.3448946475983
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
         n 750000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 62500.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 750000 x 50 data matrix successfully. X[0,0] = -0.435795
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 750000, K: 90, N*K = 67500000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (8 threads):
[===========================================================>] 99% 121.892s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000195)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 36.33 seconds), cost 9.645223
Iteration 100 (50 iterations in 36.82 seconds), cost 8.283045
Iteration 150 (50 iterations in 36.64 seconds), cost 7.952070
Iteration 200 (50 iterations in 36.39 seconds), cost 7.826298
Iteration 250 (50 iterations in 37.04 seconds), cost 7.761771
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 36.76 seconds), cost 6.417663
Iteration 350 (50 iterations in 36.63 seconds), cost 5.852353
Iteration 400 (50 iterations in 38.30 seconds), cost 5.549439
Iteration 450 (50 iterations in 38.81 seconds), cost 5.354682
Iteration 500 (50 iterations in 39.42 seconds), cost 5.214959
Iteration 550 (50 iterations in 39.89 seconds), cost 5.112976
Iteration 600 (50 iterations in 41.35 seconds), cost 5.033804
Iteration 650 (50 iterations in 43.01 seconds), cost 4.970336
Iteration 700 (50 iterations in 43.85 seconds), cost 4.919165
Iteration 750 (50 iterations in 45.80 seconds), cost 4.875578
Iteration 800 (50 iterations in 47.41 seconds), cost 4.837511
Iteration 850 (50 iterations in 48.78 seconds), cost 4.804342
Iteration 900 (50 iterations in 58.17 seconds), cost 4.777016
Iteration 950 (50 iterations in 62.81 seconds), cost 4.753345
Iteration 1000 (50 iterations in 63.55 seconds), cost 4.734463
Wrote the 750000 x 2 data matrix successfully.
Done.

FIt-SNE: 1113.3880634307861
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
         n 750000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 62500.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 750000 x 50 data matrix successfully. X[0,0] = -2.506594
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 750000, K: 90, N*K = 67500000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (8 threads):
[============================================================] 100% 117.901s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000195)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 35.56 seconds), cost 9.655788
Iteration 100 (50 iterations in 37.02 seconds), cost 8.290904
Iteration 150 (50 iterations in 36.53 seconds), cost 7.959754
Iteration 200 (50 iterations in 36.35 seconds), cost 7.833639
Iteration 250 (50 iterations in 35.94 seconds), cost 7.763984
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 36.53 seconds), cost 6.408549
Iteration 350 (50 iterations in 36.72 seconds), cost 5.840247
Iteration 400 (50 iterations in 37.13 seconds), cost 5.533632
Iteration 450 (50 iterations in 38.26 seconds), cost 5.339755
Iteration 500 (50 iterations in 39.57 seconds), cost 5.204677
Iteration 550 (50 iterations in 39.71 seconds), cost 5.104289
Iteration 600 (50 iterations in 41.05 seconds), cost 5.023952
Iteration 650 (50 iterations in 41.88 seconds), cost 4.961286
Iteration 700 (50 iterations in 43.99 seconds), cost 4.908168
Iteration 750 (50 iterations in 44.01 seconds), cost 4.864692
Iteration 800 (50 iterations in 47.51 seconds), cost 4.826333
Iteration 850 (50 iterations in 47.94 seconds), cost 4.794594
Iteration 900 (50 iterations in 55.42 seconds), cost 4.765503
Iteration 950 (50 iterations in 60.07 seconds), cost 4.742766
Iteration 1000 (50 iterations in 64.65 seconds), cost 4.724990
Wrote the 750000 x 2 data matrix successfully.
Done.

FIt-SNE: 1097.108229637146
