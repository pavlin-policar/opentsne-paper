--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 1000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 200.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.332269
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 0.258s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.135664)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.41 seconds), cost 3.242551
Iteration 100 (50 iterations in 0.41 seconds), cost 2.856734
Iteration 150 (50 iterations in 0.41 seconds), cost 2.854204
Iteration 200 (50 iterations in 0.42 seconds), cost 2.854146
Iteration 250 (50 iterations in 0.41 seconds), cost 2.854144
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.41 seconds), cost 1.029430
Iteration 350 (50 iterations in 0.41 seconds), cost 0.877862
Iteration 400 (50 iterations in 0.42 seconds), cost 0.836466
Iteration 450 (50 iterations in 0.57 seconds), cost 0.822575
Iteration 500 (50 iterations in 0.69 seconds), cost 0.815163
Iteration 550 (50 iterations in 0.71 seconds), cost 0.812733
Iteration 600 (50 iterations in 0.80 seconds), cost 0.807584
Iteration 650 (50 iterations in 0.86 seconds), cost 0.806634
Iteration 700 (50 iterations in 0.85 seconds), cost 0.805721
Iteration 750 (50 iterations in 0.92 seconds), cost 0.803571
Iteration 800 (50 iterations in 0.98 seconds), cost 0.801856
Iteration 850 (50 iterations in 0.98 seconds), cost 0.801995
Iteration 900 (50 iterations in 0.98 seconds), cost 0.800278
Iteration 950 (50 iterations in 0.99 seconds), cost 0.798168
Iteration 1000 (50 iterations in 1.08 seconds), cost 0.797999
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 14.09644627571106
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 1000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 200.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.111997
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 0.259s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.136508)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.41 seconds), cost 3.248126
Iteration 100 (50 iterations in 0.43 seconds), cost 2.874449
Iteration 150 (50 iterations in 0.42 seconds), cost 2.872624
Iteration 200 (50 iterations in 0.41 seconds), cost 2.872543
Iteration 250 (50 iterations in 0.41 seconds), cost 2.872528
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.41 seconds), cost 1.121822
Iteration 350 (50 iterations in 0.41 seconds), cost 0.962587
Iteration 400 (50 iterations in 0.48 seconds), cost 0.913770
Iteration 450 (50 iterations in 0.69 seconds), cost 0.898413
Iteration 500 (50 iterations in 0.83 seconds), cost 0.889460
Iteration 550 (50 iterations in 0.85 seconds), cost 0.886264
Iteration 600 (50 iterations in 0.97 seconds), cost 0.882070
Iteration 650 (50 iterations in 0.97 seconds), cost 0.879777
Iteration 700 (50 iterations in 0.98 seconds), cost 0.876939
Iteration 750 (50 iterations in 0.98 seconds), cost 0.874826
Iteration 800 (50 iterations in 1.08 seconds), cost 0.874351
Iteration 850 (50 iterations in 1.08 seconds), cost 0.873617
Iteration 900 (50 iterations in 1.08 seconds), cost 0.873057
Iteration 950 (50 iterations in 1.08 seconds), cost 0.871876
Iteration 1000 (50 iterations in 1.08 seconds), cost 0.870718
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 15.433695316314697
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 1000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 200.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.244387
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 0.256s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134192)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.42 seconds), cost 3.246749
Iteration 100 (50 iterations in 0.43 seconds), cost 2.768404
Iteration 150 (50 iterations in 0.42 seconds), cost 2.760278
Iteration 200 (50 iterations in 0.41 seconds), cost 2.758507
Iteration 250 (50 iterations in 0.42 seconds), cost 2.757899
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.42 seconds), cost 1.013963
Iteration 350 (50 iterations in 0.41 seconds), cost 0.863897
Iteration 400 (50 iterations in 0.46 seconds), cost 0.817901
Iteration 450 (50 iterations in 0.66 seconds), cost 0.806511
Iteration 500 (50 iterations in 0.77 seconds), cost 0.800575
Iteration 550 (50 iterations in 0.85 seconds), cost 0.797807
Iteration 600 (50 iterations in 0.87 seconds), cost 0.792944
Iteration 650 (50 iterations in 0.99 seconds), cost 0.791355
Iteration 700 (50 iterations in 0.98 seconds), cost 0.788927
Iteration 750 (50 iterations in 0.98 seconds), cost 0.786437
Iteration 800 (50 iterations in 1.32 seconds), cost 0.784674
Iteration 850 (50 iterations in 1.13 seconds), cost 0.783973
Iteration 900 (50 iterations in 1.08 seconds), cost 0.782603
Iteration 950 (50 iterations in 1.09 seconds), cost 0.782443
Iteration 1000 (50 iterations in 1.14 seconds), cost 0.780084
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 15.636429071426392
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 1000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 200.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -2.861498
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 0.254s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.135664)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.43 seconds), cost 3.145184
Iteration 100 (50 iterations in 0.43 seconds), cost 2.829102
Iteration 150 (50 iterations in 0.43 seconds), cost 2.827290
Iteration 200 (50 iterations in 0.41 seconds), cost 2.827327
Iteration 250 (50 iterations in 0.41 seconds), cost 2.827335
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.40 seconds), cost 1.082777
Iteration 350 (50 iterations in 0.41 seconds), cost 0.927184
Iteration 400 (50 iterations in 0.42 seconds), cost 0.881262
Iteration 450 (50 iterations in 0.51 seconds), cost 0.863861
Iteration 500 (50 iterations in 0.62 seconds), cost 0.857547
Iteration 550 (50 iterations in 0.71 seconds), cost 0.854903
Iteration 600 (50 iterations in 0.71 seconds), cost 0.852761
Iteration 650 (50 iterations in 0.72 seconds), cost 0.849294
Iteration 700 (50 iterations in 0.85 seconds), cost 0.849563
Iteration 750 (50 iterations in 0.85 seconds), cost 0.848350
Iteration 800 (50 iterations in 0.85 seconds), cost 0.847635
Iteration 850 (50 iterations in 0.85 seconds), cost 0.846742
Iteration 900 (50 iterations in 0.85 seconds), cost 0.845388
Iteration 950 (50 iterations in 0.95 seconds), cost 0.842781
Iteration 1000 (50 iterations in 1.00 seconds), cost 0.842980
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 13.169396877288818
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
         n 1000 by d 50 dataset, theta 0.500000,
         perplexity 30.000000, no_dims 2, max_iter 1000,
         stop_lying_iter 250, mom_switch_iter 250,
         momentum 0.500000, final_momentum 0.800000,
         learning_rate 200.000000, max_step_norm 5.000000,
         K -1, sigma -1.000000, nbody_algo 2,
         knn_algo 1, early_exag_coeff 12.000000,
         no_momentum_during_exag 0, n_trees 50, search_k 4500,
         start_late_exag_iter -1, late_exag_coeff -1.000000
         nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.439911
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search...
parallel (1 threads):
[============================================================] 100% 0.26s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134708)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.41 seconds), cost 3.280668
Iteration 100 (50 iterations in 0.44 seconds), cost 2.823807
Iteration 150 (50 iterations in 0.41 seconds), cost 2.816658
Iteration 200 (50 iterations in 0.42 seconds), cost 2.816614
Iteration 250 (50 iterations in 0.41 seconds), cost 2.816613
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.41 seconds), cost 1.053692
Iteration 350 (50 iterations in 0.42 seconds), cost 0.895273
Iteration 400 (50 iterations in 0.44 seconds), cost 0.849420
Iteration 450 (50 iterations in 0.65 seconds), cost 0.834861
Iteration 500 (50 iterations in 0.71 seconds), cost 0.828722
Iteration 550 (50 iterations in 0.85 seconds), cost 0.822513
Iteration 600 (50 iterations in 0.85 seconds), cost 0.819906
Iteration 650 (50 iterations in 0.96 seconds), cost 0.816520
Iteration 700 (50 iterations in 0.98 seconds), cost 0.814122
Iteration 750 (50 iterations in 0.98 seconds), cost 0.814413
Iteration 800 (50 iterations in 0.99 seconds), cost 0.812402
Iteration 850 (50 iterations in 1.06 seconds), cost 0.809479
Iteration 900 (50 iterations in 1.09 seconds), cost 0.809494
Iteration 950 (50 iterations in 1.09 seconds), cost 0.807480
Iteration 1000 (50 iterations in 1.09 seconds), cost 0.807173
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 15.071176528930664
