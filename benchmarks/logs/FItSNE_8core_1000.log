--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data_2021-11-20 10:26:44.899287-322983876.dat
fast_tsne result_path: result_2021-11-20 10:26:44.899287-322983876.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 750,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, max_step_norm 5.000000,
	 K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.037486
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[>                                                           ] 0% 0.001s[======================================================>     ] 90% 0.037s[============================================================] 100% 0.044s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.132384)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.51 seconds), cost 3.241853
Iteration 100 (50 iterations in 0.50 seconds), cost 2.705177
Iteration 150 (50 iterations in 0.51 seconds), cost 2.698399
Iteration 200 (50 iterations in 0.47 seconds), cost 2.694129
Iteration 250 (50 iterations in 0.51 seconds), cost 2.658897
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.48 seconds), cost 1.035576
Iteration 350 (50 iterations in 0.51 seconds), cost 0.875616
Iteration 400 (50 iterations in 0.51 seconds), cost 0.837732
Iteration 450 (50 iterations in 0.75 seconds), cost 0.822741
Iteration 500 (50 iterations in 0.81 seconds), cost 0.819630
Iteration 550 (50 iterations in 0.85 seconds), cost 0.819161
Iteration 600 (50 iterations in 0.86 seconds), cost 0.812462
Iteration 650 (50 iterations in 0.96 seconds), cost 0.802095
Iteration 700 (50 iterations in 0.98 seconds), cost 0.810385
Iteration 750 (50 iterations in 1.04 seconds), cost 0.806002
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 10.399316549301147
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data_2021-11-20 10:26:59.014215-130427262.dat
fast_tsne result_path: result_2021-11-20 10:26:59.014215-130427262.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 750,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, max_step_norm 5.000000,
	 K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.772621
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[>                                                           ] 0% 0.001s[======================================================>     ] 90% 0.04s[===========================================================>] 99% 0.047s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.133120)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.50 seconds), cost 3.218878
Iteration 100 (50 iterations in 0.48 seconds), cost 2.775834
Iteration 150 (50 iterations in 0.48 seconds), cost 2.796483
Iteration 200 (50 iterations in 0.48 seconds), cost 2.785441
Iteration 250 (50 iterations in 0.49 seconds), cost 2.792057
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.50 seconds), cost 1.024961
Iteration 350 (50 iterations in 0.49 seconds), cost 0.873994
Iteration 400 (50 iterations in 0.56 seconds), cost 0.833568
Iteration 450 (50 iterations in 0.72 seconds), cost 0.811265
Iteration 500 (50 iterations in 0.88 seconds), cost 0.807883
Iteration 550 (50 iterations in 0.86 seconds), cost 0.808583
Iteration 600 (50 iterations in 0.95 seconds), cost 0.805769
Iteration 650 (50 iterations in 0.95 seconds), cost 0.802137
Iteration 700 (50 iterations in 0.98 seconds), cost 0.794131
Iteration 750 (50 iterations in 1.04 seconds), cost 0.797160
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 10.527079820632935
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data_2021-11-20 10:27:13.258746-238019840.dat
fast_tsne result_path: result_2021-11-20 10:27:13.258746-238019840.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 750,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, max_step_norm 5.000000,
	 K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.180971
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[>                                                           ] 0% 0.001s[===================================================>        ] 85% 0.036s[===========================================================>] 99% 0.044s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134050)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.49 seconds), cost 3.241724
Iteration 100 (50 iterations in 0.49 seconds), cost 2.796860
Iteration 150 (50 iterations in 0.51 seconds), cost 2.813425
Iteration 200 (50 iterations in 0.51 seconds), cost 2.792571
Iteration 250 (50 iterations in 0.56 seconds), cost 2.781777
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.49 seconds), cost 1.015639
Iteration 350 (50 iterations in 0.48 seconds), cost 0.864247
Iteration 400 (50 iterations in 0.50 seconds), cost 0.816695
Iteration 450 (50 iterations in 0.59 seconds), cost 0.797011
Iteration 500 (50 iterations in 0.66 seconds), cost 0.796022
Iteration 550 (50 iterations in 0.74 seconds), cost 0.792212
Iteration 600 (50 iterations in 0.75 seconds), cost 0.789903
Iteration 650 (50 iterations in 0.77 seconds), cost 0.789239
Iteration 700 (50 iterations in 0.83 seconds), cost 0.779261
Iteration 750 (50 iterations in 0.80 seconds), cost 0.784822
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 9.323293685913086
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data_2021-11-20 10:27:26.270640-943132465.dat
fast_tsne result_path: result_2021-11-20 10:27:26.270640-943132465.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 750,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, max_step_norm 5.000000,
	 K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.849021
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[>                                                           ] 0% 0s[====================================>                       ] 61% 0.028s[===========================================================>] 99% 0.045s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134558)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.50 seconds), cost 3.252936
Iteration 100 (50 iterations in 0.49 seconds), cost 2.842158
Iteration 150 (50 iterations in 0.51 seconds), cost 2.831959
Iteration 200 (50 iterations in 0.47 seconds), cost 2.836161
Iteration 250 (50 iterations in 0.47 seconds), cost 2.794777
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.50 seconds), cost 1.066425
Iteration 350 (50 iterations in 0.49 seconds), cost 0.909971
Iteration 400 (50 iterations in 0.53 seconds), cost 0.860010
Iteration 450 (50 iterations in 0.60 seconds), cost 0.843897
Iteration 500 (50 iterations in 0.64 seconds), cost 0.832604
Iteration 550 (50 iterations in 0.73 seconds), cost 0.834236
Iteration 600 (50 iterations in 0.75 seconds), cost 0.820747
Iteration 650 (50 iterations in 0.81 seconds), cost 0.828867
Iteration 700 (50 iterations in 0.82 seconds), cost 0.818115
Iteration 750 (50 iterations in 0.81 seconds), cost 0.818075
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 9.27440857887268
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data_2021-11-20 10:27:39.243362-520454900.dat
fast_tsne result_path: result_2021-11-20 10:27:39.243362-520454900.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 750,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, max_step_norm 5.000000,
	 K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -1.262063
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[>                                                           ] 0% 0s[==============================================>             ] 76% 0.03s[============================================================] 100% 0.046s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.132408)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.52 seconds), cost 3.168272
Iteration 100 (50 iterations in 0.48 seconds), cost 2.752915
Iteration 150 (50 iterations in 0.47 seconds), cost 2.738005
Iteration 200 (50 iterations in 0.54 seconds), cost 2.729091
Iteration 250 (50 iterations in 0.47 seconds), cost 2.733761
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.48 seconds), cost 1.013985
Iteration 350 (50 iterations in 0.49 seconds), cost 0.864330
Iteration 400 (50 iterations in 0.54 seconds), cost 0.821534
Iteration 450 (50 iterations in 0.66 seconds), cost 0.809911
Iteration 500 (50 iterations in 0.73 seconds), cost 0.801205
Iteration 550 (50 iterations in 0.85 seconds), cost 0.793727
Iteration 600 (50 iterations in 0.81 seconds), cost 0.793598
Iteration 650 (50 iterations in 0.93 seconds), cost 0.790576
Iteration 700 (50 iterations in 0.90 seconds), cost 0.789325
Iteration 750 (50 iterations in 0.95 seconds), cost 0.786279
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 9.966316938400269
--------------------------------------------------------------------------------
Random state 5
--------------------------------------------------------------------------------
=============== t-SNE v1.2.1 ===============
fast_tsne data_path: data_2021-11-20 10:27:52.902397-748710742.dat
fast_tsne result_path: result_2021-11-20 10:27:52.902397-748710742.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 750,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, max_step_norm 5.000000,
	 K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = 4.467943
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[>                                                           ] 0% 0s[======================================>                     ] 63% 0.028s[============================================================] 100% 0.044s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134816)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.53 seconds), cost 3.233388
Iteration 100 (50 iterations in 0.47 seconds), cost 2.756582
Iteration 150 (50 iterations in 0.50 seconds), cost 2.766835
Iteration 200 (50 iterations in 0.50 seconds), cost 2.730856
Iteration 250 (50 iterations in 0.48 seconds), cost 2.756963
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.51 seconds), cost 0.999196
Iteration 350 (50 iterations in 0.50 seconds), cost 0.843832
Iteration 400 (50 iterations in 0.50 seconds), cost 0.799393
Iteration 450 (50 iterations in 0.59 seconds), cost 0.784016
Iteration 500 (50 iterations in 0.64 seconds), cost 0.778497
Iteration 550 (50 iterations in 0.74 seconds), cost 0.776411
Iteration 600 (50 iterations in 0.72 seconds), cost 0.774024
Iteration 650 (50 iterations in 0.73 seconds), cost 0.771209
Iteration 700 (50 iterations in 0.77 seconds), cost 0.765812
Iteration 750 (50 iterations in 0.81 seconds), cost 0.769139
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 9.16376781463623
