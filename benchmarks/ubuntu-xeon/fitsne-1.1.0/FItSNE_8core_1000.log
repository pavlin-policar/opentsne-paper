--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = 3.365040
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[>                                                           ] 0% 0.001s
[=================================================>          ] 83% 0.061s
[============================================================] 100% 0.073s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.133974)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.88 seconds), cost 3.208624
Iteration 100 (50 iterations in 0.88 seconds), cost 2.856053
Iteration 150 (50 iterations in 0.90 seconds), cost 2.778377
Iteration 200 (50 iterations in 0.93 seconds), cost 2.754262
Iteration 250 (50 iterations in 0.89 seconds), cost 2.750951
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.93 seconds), cost 1.005224
Iteration 350 (50 iterations in 0.95 seconds), cost 0.854746
Iteration 400 (50 iterations in 0.91 seconds), cost 0.809464
Iteration 450 (50 iterations in 1.40 seconds), cost 0.805162
Iteration 500 (50 iterations in 1.66 seconds), cost 0.797534
Iteration 550 (50 iterations in 1.85 seconds), cost 0.792951
Iteration 600 (50 iterations in 1.74 seconds), cost 0.782820
Iteration 650 (50 iterations in 2.06 seconds), cost 0.785208
Iteration 700 (50 iterations in 2.14 seconds), cost 0.786396
Iteration 750 (50 iterations in 2.10 seconds), cost 0.783333
Iteration 800 (50 iterations in 2.29 seconds), cost 0.781714
Iteration 850 (50 iterations in 2.39 seconds), cost 0.780989
Iteration 900 (50 iterations in 2.45 seconds), cost 0.782637
Iteration 950 (50 iterations in 2.41 seconds), cost 0.781604
Iteration 1000 (50 iterations in 2.36 seconds), cost 0.777846
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 32.43043828010559
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -4.143700
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[>                                                           ] 0% 0.001s
[=================================================>          ] 82% 0.06s
[============================================================] 100% 0.072s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134494)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.98 seconds), cost 3.183128
Iteration 100 (50 iterations in 0.92 seconds), cost 2.778286
Iteration 150 (50 iterations in 0.84 seconds), cost 2.762772
Iteration 200 (50 iterations in 0.69 seconds), cost 2.722114
Iteration 250 (50 iterations in 0.89 seconds), cost 2.775720
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.93 seconds), cost 1.030251
Iteration 350 (50 iterations in 1.00 seconds), cost 0.884650
Iteration 400 (50 iterations in 1.00 seconds), cost 0.834892
Iteration 450 (50 iterations in 1.53 seconds), cost 0.829523
Iteration 500 (50 iterations in 1.78 seconds), cost 0.816087
Iteration 550 (50 iterations in 1.88 seconds), cost 0.809430
Iteration 600 (50 iterations in 1.97 seconds), cost 0.815187
Iteration 650 (50 iterations in 2.10 seconds), cost 0.808464
Iteration 700 (50 iterations in 2.15 seconds), cost 0.808270
Iteration 750 (50 iterations in 2.09 seconds), cost 0.800905
Iteration 800 (50 iterations in 2.33 seconds), cost 0.801062
Iteration 850 (50 iterations in 2.40 seconds), cost 0.803678
Iteration 900 (50 iterations in 2.35 seconds), cost 0.799874
Iteration 950 (50 iterations in 2.51 seconds), cost 0.805799
Iteration 1000 (50 iterations in 2.38 seconds), cost 0.802424
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 32.96933341026306
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.681004
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[>                                                           ] 0% 0.001s
[==============================================>             ] 78% 0.06s
[===========================================================>] 100% 0.08s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134916)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.97 seconds), cost 3.179213
Iteration 100 (50 iterations in 0.94 seconds), cost 2.790174
Iteration 150 (50 iterations in 0.88 seconds), cost 2.825861
Iteration 200 (50 iterations in 0.90 seconds), cost 2.841890
Iteration 250 (50 iterations in 0.98 seconds), cost 2.830206
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.98 seconds), cost 1.051541
Iteration 350 (50 iterations in 0.90 seconds), cost 0.895068
Iteration 400 (50 iterations in 1.00 seconds), cost 0.855176
Iteration 450 (50 iterations in 1.32 seconds), cost 0.834224
Iteration 500 (50 iterations in 1.64 seconds), cost 0.829514
Iteration 550 (50 iterations in 1.88 seconds), cost 0.828240
Iteration 600 (50 iterations in 1.88 seconds), cost 0.825299
Iteration 650 (50 iterations in 2.07 seconds), cost 0.816135
Iteration 700 (50 iterations in 2.17 seconds), cost 0.811161
Iteration 750 (50 iterations in 2.20 seconds), cost 0.812738
Iteration 800 (50 iterations in 2.15 seconds), cost 0.811070
Iteration 850 (50 iterations in 2.26 seconds), cost 0.810908
Iteration 900 (50 iterations in 2.24 seconds), cost 0.808842
Iteration 950 (50 iterations in 2.36 seconds), cost 0.805427
Iteration 1000 (50 iterations in 2.47 seconds), cost 0.807074
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 32.46304488182068
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.911167
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[>                                                           ] 0% 0.001s
[================================================>           ] 80% 0.06s
[============================================================] 100% 0.072s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134448)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 1.02 seconds), cost 3.248378
Iteration 100 (50 iterations in 0.95 seconds), cost 2.821416
Iteration 150 (50 iterations in 0.90 seconds), cost 2.770038
Iteration 200 (50 iterations in 0.99 seconds), cost 2.797874
Iteration 250 (50 iterations in 0.86 seconds), cost 2.766282
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.98 seconds), cost 1.067081
Iteration 350 (50 iterations in 0.94 seconds), cost 0.913211
Iteration 400 (50 iterations in 0.83 seconds), cost 0.857333
Iteration 450 (50 iterations in 1.19 seconds), cost 0.845233
Iteration 500 (50 iterations in 1.35 seconds), cost 0.845298
Iteration 550 (50 iterations in 1.58 seconds), cost 0.834532
Iteration 600 (50 iterations in 1.64 seconds), cost 0.837926
Iteration 650 (50 iterations in 1.73 seconds), cost 0.836268
Iteration 700 (50 iterations in 1.68 seconds), cost 0.830412
Iteration 750 (50 iterations in 1.82 seconds), cost 0.830618
Iteration 800 (50 iterations in 1.98 seconds), cost 0.832094
Iteration 850 (50 iterations in 1.92 seconds), cost 0.822113
Iteration 900 (50 iterations in 1.93 seconds), cost 0.829763
Iteration 950 (50 iterations in 1.96 seconds), cost 0.825654
Iteration 1000 (50 iterations in 1.96 seconds), cost 0.822106
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 28.453466653823853
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -2.834157
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[>                                                           ] 0% 0.001s
[=================================================>          ] 82% 0.059s
[============================================================] 100% 0.072s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134406)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 1.04 seconds), cost 3.276579
Iteration 100 (50 iterations in 0.96 seconds), cost 2.799727
Iteration 150 (50 iterations in 1.03 seconds), cost 2.783560
Iteration 200 (50 iterations in 0.96 seconds), cost 2.784085
Iteration 250 (50 iterations in 0.93 seconds), cost 2.775048
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 1.01 seconds), cost 1.006272
Iteration 350 (50 iterations in 0.97 seconds), cost 0.855649
Iteration 400 (50 iterations in 0.99 seconds), cost 0.809900
Iteration 450 (50 iterations in 1.63 seconds), cost 0.794674
Iteration 500 (50 iterations in 1.73 seconds), cost 0.786759
Iteration 550 (50 iterations in 1.78 seconds), cost 0.788188
Iteration 600 (50 iterations in 2.00 seconds), cost 0.782604
Iteration 650 (50 iterations in 2.08 seconds), cost 0.779595
Iteration 700 (50 iterations in 2.20 seconds), cost 0.779778
Iteration 750 (50 iterations in 1.70 seconds), cost 0.780616
Iteration 800 (50 iterations in 1.93 seconds), cost 0.775985
Iteration 850 (50 iterations in 2.42 seconds), cost 0.766051
Iteration 900 (50 iterations in 2.30 seconds), cost 0.772191
Iteration 950 (50 iterations in 2.39 seconds), cost 0.769213
Iteration 1000 (50 iterations in 2.32 seconds), cost 0.762069
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 32.60773992538452
