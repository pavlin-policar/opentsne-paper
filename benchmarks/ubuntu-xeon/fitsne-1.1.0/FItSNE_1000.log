--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.905703
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 0.363s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.133144)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.59 seconds), cost 3.200753
Iteration 100 (50 iterations in 0.56 seconds), cost 2.773473
Iteration 150 (50 iterations in 0.55 seconds), cost 2.763324
Iteration 200 (50 iterations in 0.55 seconds), cost 2.763218
Iteration 250 (50 iterations in 0.55 seconds), cost 2.763222
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.55 seconds), cost 1.048305
Iteration 350 (50 iterations in 0.55 seconds), cost 0.899580
Iteration 400 (50 iterations in 0.55 seconds), cost 0.854070
Iteration 450 (50 iterations in 0.77 seconds), cost 0.839422
Iteration 500 (50 iterations in 0.89 seconds), cost 0.833136
Iteration 550 (50 iterations in 0.98 seconds), cost 0.830397
Iteration 600 (50 iterations in 0.98 seconds), cost 0.827474
Iteration 650 (50 iterations in 1.09 seconds), cost 0.823240
Iteration 700 (50 iterations in 1.12 seconds), cost 0.821938
Iteration 750 (50 iterations in 1.12 seconds), cost 0.820583
Iteration 800 (50 iterations in 1.12 seconds), cost 0.819623
Iteration 850 (50 iterations in 1.20 seconds), cost 0.817102
Iteration 900 (50 iterations in 1.28 seconds), cost 0.817216
Iteration 950 (50 iterations in 1.28 seconds), cost 0.816581
Iteration 1000 (50 iterations in 1.28 seconds), cost 0.815550
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 18.15751624107361
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.492149
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 0.374s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.133406)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.59 seconds), cost 3.221525
Iteration 100 (50 iterations in 0.56 seconds), cost 2.758672
Iteration 150 (50 iterations in 0.59 seconds), cost 2.753475
Iteration 200 (50 iterations in 0.56 seconds), cost 2.753153
Iteration 250 (50 iterations in 0.56 seconds), cost 2.752922
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.56 seconds), cost 1.017374
Iteration 350 (50 iterations in 0.56 seconds), cost 0.859332
Iteration 400 (50 iterations in 0.68 seconds), cost 0.814222
Iteration 450 (50 iterations in 0.94 seconds), cost 0.798588
Iteration 500 (50 iterations in 1.06 seconds), cost 0.789922
Iteration 550 (50 iterations in 1.12 seconds), cost 0.785764
Iteration 600 (50 iterations in 1.26 seconds), cost 0.780133
Iteration 650 (50 iterations in 1.28 seconds), cost 0.778393
Iteration 700 (50 iterations in 1.32 seconds), cost 0.776348
Iteration 750 (50 iterations in 1.41 seconds), cost 0.774665
Iteration 800 (50 iterations in 1.41 seconds), cost 0.773814
Iteration 850 (50 iterations in 1.41 seconds), cost 0.773116
Iteration 900 (50 iterations in 1.42 seconds), cost 0.770402
Iteration 950 (50 iterations in 1.44 seconds), cost 0.770102
Iteration 1000 (50 iterations in 1.44 seconds), cost 0.768409
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 20.74163269996643
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.265549
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 0.374s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.135776)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.59 seconds), cost 3.222952
Iteration 100 (50 iterations in 0.56 seconds), cost 2.915585
Iteration 150 (50 iterations in 0.56 seconds), cost 2.870742
Iteration 200 (50 iterations in 0.56 seconds), cost 2.870485
Iteration 250 (50 iterations in 0.56 seconds), cost 2.870483
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.56 seconds), cost 1.090885
Iteration 350 (50 iterations in 0.56 seconds), cost 0.933453
Iteration 400 (50 iterations in 0.56 seconds), cost 0.887301
Iteration 450 (50 iterations in 0.79 seconds), cost 0.871018
Iteration 500 (50 iterations in 0.98 seconds), cost 0.865805
Iteration 550 (50 iterations in 1.04 seconds), cost 0.859864
Iteration 600 (50 iterations in 1.12 seconds), cost 0.857350
Iteration 650 (50 iterations in 1.12 seconds), cost 0.856242
Iteration 700 (50 iterations in 1.12 seconds), cost 0.855131
Iteration 750 (50 iterations in 1.28 seconds), cost 0.853128
Iteration 800 (50 iterations in 1.28 seconds), cost 0.852343
Iteration 850 (50 iterations in 1.28 seconds), cost 0.851527
Iteration 900 (50 iterations in 1.28 seconds), cost 0.850867
Iteration 950 (50 iterations in 1.28 seconds), cost 0.850863
Iteration 1000 (50 iterations in 1.36 seconds), cost 0.846401
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 19.027384519577026
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = -3.204340
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 0.373s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.134760)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.60 seconds), cost 3.278512
Iteration 100 (50 iterations in 0.57 seconds), cost 2.850643
Iteration 150 (50 iterations in 0.56 seconds), cost 2.843906
Iteration 200 (50 iterations in 0.56 seconds), cost 2.843782
Iteration 250 (50 iterations in 0.56 seconds), cost 2.843775
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.56 seconds), cost 1.070810
Iteration 350 (50 iterations in 0.56 seconds), cost 0.912229
Iteration 400 (50 iterations in 0.62 seconds), cost 0.866450
Iteration 450 (50 iterations in 0.93 seconds), cost 0.851760
Iteration 500 (50 iterations in 1.03 seconds), cost 0.844260
Iteration 550 (50 iterations in 1.12 seconds), cost 0.840766
Iteration 600 (50 iterations in 1.17 seconds), cost 0.835819
Iteration 650 (50 iterations in 1.28 seconds), cost 0.834185
Iteration 700 (50 iterations in 1.29 seconds), cost 0.832411
Iteration 750 (50 iterations in 1.28 seconds), cost 0.831265
Iteration 800 (50 iterations in 1.34 seconds), cost 0.828432
Iteration 850 (50 iterations in 1.40 seconds), cost 0.828063
Iteration 900 (50 iterations in 1.41 seconds), cost 0.827682
Iteration 950 (50 iterations in 1.41 seconds), cost 0.826988
Iteration 1000 (50 iterations in 1.40 seconds), cost 0.826169
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 20.21633267402649
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 1000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000 x 50 data matrix successfully. X[0,0] = 0.892197
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000, K: 90, N*K = 90000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 0.368s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.133538)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 0.59 seconds), cost 3.188823
Iteration 100 (50 iterations in 0.57 seconds), cost 2.741368
Iteration 150 (50 iterations in 0.57 seconds), cost 2.732704
Iteration 200 (50 iterations in 0.57 seconds), cost 2.731582
Iteration 250 (50 iterations in 0.56 seconds), cost 2.731281
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 0.56 seconds), cost 1.023157
Iteration 350 (50 iterations in 0.56 seconds), cost 0.873136
Iteration 400 (50 iterations in 0.56 seconds), cost 0.830882
Iteration 450 (50 iterations in 0.73 seconds), cost 0.815851
Iteration 500 (50 iterations in 0.82 seconds), cost 0.809776
Iteration 550 (50 iterations in 0.99 seconds), cost 0.807961
Iteration 600 (50 iterations in 0.98 seconds), cost 0.805805
Iteration 650 (50 iterations in 0.98 seconds), cost 0.804590
Iteration 700 (50 iterations in 1.07 seconds), cost 0.799067
Iteration 750 (50 iterations in 1.12 seconds), cost 0.799407
Iteration 800 (50 iterations in 1.12 seconds), cost 0.797461
Iteration 850 (50 iterations in 1.12 seconds), cost 0.796947
Iteration 900 (50 iterations in 1.12 seconds), cost 0.795331
Iteration 950 (50 iterations in 1.19 seconds), cost 0.792057
Iteration 1000 (50 iterations in 1.27 seconds), cost 0.791921
Wrote the 1000 x 2 data matrix successfully.
Done.

FIt-SNE: 17.568268060684204
