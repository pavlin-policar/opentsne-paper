--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 500000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 500000 x 50 data matrix successfully. X[0,0] = -0.442204
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 500000, K: 90, N*K = 45000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 457.355s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000291)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 53.39 seconds), cost 9.345078
Iteration 100 (50 iterations in 50.86 seconds), cost 9.345078
Iteration 150 (50 iterations in 50.80 seconds), cost 9.345078
Iteration 200 (50 iterations in 50.61 seconds), cost 9.345078
Iteration 250 (50 iterations in 50.50 seconds), cost 9.345078
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 50.79 seconds), cost 9.345070
Iteration 350 (50 iterations in 50.66 seconds), cost 9.318468
Iteration 400 (50 iterations in 51.54 seconds), cost 8.480384
Iteration 450 (50 iterations in 52.00 seconds), cost 7.804503
Iteration 500 (50 iterations in 52.00 seconds), cost 7.448960
Iteration 550 (50 iterations in 52.07 seconds), cost 7.206212
Iteration 600 (50 iterations in 52.11 seconds), cost 7.019020
Iteration 650 (50 iterations in 51.97 seconds), cost 6.865739
Iteration 700 (50 iterations in 51.94 seconds), cost 6.735687
Iteration 750 (50 iterations in 52.11 seconds), cost 6.622592
Iteration 800 (50 iterations in 52.11 seconds), cost 6.522295
Iteration 850 (50 iterations in 52.10 seconds), cost 6.432254
Iteration 900 (50 iterations in 51.99 seconds), cost 6.350846
Iteration 950 (50 iterations in 52.07 seconds), cost 6.276808
Iteration 1000 (50 iterations in 51.97 seconds), cost 6.209173
Wrote the 500000 x 2 data matrix successfully.
Done.

FIt-SNE: 1582.0892543792725
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 500000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 500000 x 50 data matrix successfully. X[0,0] = -2.271639
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 500000, K: 90, N*K = 45000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 458.191s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000291)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 53.74 seconds), cost 9.345488
Iteration 100 (50 iterations in 50.83 seconds), cost 9.345488
Iteration 150 (50 iterations in 50.73 seconds), cost 9.345488
Iteration 200 (50 iterations in 50.66 seconds), cost 9.345488
Iteration 250 (50 iterations in 50.48 seconds), cost 9.345488
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 50.67 seconds), cost 9.345479
Iteration 350 (50 iterations in 50.68 seconds), cost 9.314229
Iteration 400 (50 iterations in 51.35 seconds), cost 8.367416
Iteration 450 (50 iterations in 51.83 seconds), cost 7.753197
Iteration 500 (50 iterations in 51.83 seconds), cost 7.407117
Iteration 550 (50 iterations in 51.99 seconds), cost 7.167014
Iteration 600 (50 iterations in 52.01 seconds), cost 6.980760
Iteration 650 (50 iterations in 52.01 seconds), cost 6.827807
Iteration 700 (50 iterations in 52.01 seconds), cost 6.698187
Iteration 750 (50 iterations in 51.92 seconds), cost 6.585673
Iteration 800 (50 iterations in 51.88 seconds), cost 6.486058
Iteration 850 (50 iterations in 52.05 seconds), cost 6.396705
Iteration 900 (50 iterations in 52.15 seconds), cost 6.315883
Iteration 950 (50 iterations in 51.99 seconds), cost 6.242438
Iteration 1000 (50 iterations in 51.92 seconds), cost 6.175275
Wrote the 500000 x 2 data matrix successfully.
Done.

FIt-SNE: 1581.241319656372
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 500000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 500000 x 50 data matrix successfully. X[0,0] = -2.614497
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 500000, K: 90, N*K = 45000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 456.434s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000291)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 51.40 seconds), cost 9.345518
Iteration 100 (50 iterations in 50.88 seconds), cost 9.345518
Iteration 150 (50 iterations in 50.78 seconds), cost 9.345518
Iteration 200 (50 iterations in 50.76 seconds), cost 9.345518
Iteration 250 (50 iterations in 50.73 seconds), cost 9.345518
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 50.76 seconds), cost 9.345511
Iteration 350 (50 iterations in 50.71 seconds), cost 9.317125
Iteration 400 (50 iterations in 51.47 seconds), cost 8.351097
Iteration 450 (50 iterations in 51.92 seconds), cost 7.715265
Iteration 500 (50 iterations in 52.03 seconds), cost 7.371722
Iteration 550 (50 iterations in 52.04 seconds), cost 7.133933
Iteration 600 (50 iterations in 52.06 seconds), cost 6.951712
Iteration 650 (50 iterations in 52.09 seconds), cost 6.803370
Iteration 700 (50 iterations in 52.03 seconds), cost 6.677520
Iteration 750 (50 iterations in 51.99 seconds), cost 6.567638
Iteration 800 (50 iterations in 51.99 seconds), cost 6.469992
Iteration 850 (50 iterations in 51.99 seconds), cost 6.382328
Iteration 900 (50 iterations in 52.05 seconds), cost 6.303008
Iteration 950 (50 iterations in 52.04 seconds), cost 6.230887
Iteration 1000 (50 iterations in 52.05 seconds), cost 6.165051
Wrote the 500000 x 2 data matrix successfully.
Done.

FIt-SNE: 1579.5777831077576
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 500000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 500000 x 50 data matrix successfully. X[0,0] = -2.747781
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 500000, K: 90, N*K = 45000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 457.263s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000291)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 51.28 seconds), cost 9.345670
Iteration 100 (50 iterations in 50.79 seconds), cost 9.345670
Iteration 150 (50 iterations in 50.73 seconds), cost 9.345670
Iteration 200 (50 iterations in 50.62 seconds), cost 9.345670
Iteration 250 (50 iterations in 50.55 seconds), cost 9.345670
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 50.67 seconds), cost 9.345663
Iteration 350 (50 iterations in 50.54 seconds), cost 9.319807
Iteration 400 (50 iterations in 51.51 seconds), cost 8.440676
Iteration 450 (50 iterations in 51.96 seconds), cost 7.790510
Iteration 500 (50 iterations in 52.00 seconds), cost 7.451707
Iteration 550 (50 iterations in 51.98 seconds), cost 7.218887
Iteration 600 (50 iterations in 51.99 seconds), cost 7.039169
Iteration 650 (50 iterations in 51.95 seconds), cost 6.891619
Iteration 700 (50 iterations in 51.96 seconds), cost 6.766036
Iteration 750 (50 iterations in 52.03 seconds), cost 6.656309
Iteration 800 (50 iterations in 52.04 seconds), cost 6.558624
Iteration 850 (50 iterations in 52.06 seconds), cost 6.470651
Iteration 900 (50 iterations in 52.04 seconds), cost 6.390765
Iteration 950 (50 iterations in 52.03 seconds), cost 6.317854
Iteration 1000 (50 iterations in 52.05 seconds), cost 6.250924
Wrote the 500000 x 2 data matrix successfully.
Done.

FIt-SNE: 1579.8036632537842
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 500000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 500000 x 50 data matrix successfully. X[0,0] = -3.460196
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 500000, K: 90, N*K = 45000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 457.378s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000291)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 51.32 seconds), cost 9.345385
Iteration 100 (50 iterations in 50.80 seconds), cost 9.345385
Iteration 150 (50 iterations in 50.75 seconds), cost 9.345385
Iteration 200 (50 iterations in 50.70 seconds), cost 9.345385
Iteration 250 (50 iterations in 50.69 seconds), cost 9.345385
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 50.79 seconds), cost 9.345377
Iteration 350 (50 iterations in 50.72 seconds), cost 9.320821
Iteration 400 (50 iterations in 51.62 seconds), cost 8.423979
Iteration 450 (50 iterations in 52.01 seconds), cost 7.768771
Iteration 500 (50 iterations in 52.08 seconds), cost 7.430383
Iteration 550 (50 iterations in 52.10 seconds), cost 7.199089
Iteration 600 (50 iterations in 52.13 seconds), cost 7.021312
Iteration 650 (50 iterations in 52.13 seconds), cost 6.875650
Iteration 700 (50 iterations in 52.11 seconds), cost 6.751791
Iteration 750 (50 iterations in 52.12 seconds), cost 6.643917
Iteration 800 (50 iterations in 52.12 seconds), cost 6.548302
Iteration 850 (50 iterations in 52.13 seconds), cost 6.462393
Iteration 900 (50 iterations in 52.12 seconds), cost 6.384137
Iteration 950 (50 iterations in 52.11 seconds), cost 6.312181
Iteration 1000 (50 iterations in 52.12 seconds), cost 6.245792
Wrote the 500000 x 2 data matrix successfully.
Done.

FIt-SNE: 1581.0606997013092
