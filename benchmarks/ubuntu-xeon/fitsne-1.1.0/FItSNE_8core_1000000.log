--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000000 x 50 data matrix successfully. X[0,0] = -0.637658
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000000, K: 90, N*K = 90000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 159.693s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000147)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 42.99 seconds), cost 9.777114
Iteration 100 (50 iterations in 42.45 seconds), cost 9.797600
Iteration 150 (50 iterations in 41.54 seconds), cost 9.789045
Iteration 200 (50 iterations in 41.07 seconds), cost 9.768966
Iteration 250 (50 iterations in 41.66 seconds), cost 9.801968
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 41.42 seconds), cost 9.820689
Iteration 350 (50 iterations in 41.28 seconds), cost 9.819478
Iteration 400 (50 iterations in 40.65 seconds), cost 9.833179
Iteration 450 (50 iterations in 41.03 seconds), cost 9.817071
Iteration 500 (50 iterations in 42.84 seconds), cost 9.263230
Iteration 550 (50 iterations in 45.07 seconds), cost 8.684411
Iteration 600 (50 iterations in 44.21 seconds), cost 8.307503
Iteration 650 (50 iterations in 44.13 seconds), cost 8.091289
Iteration 700 (50 iterations in 44.31 seconds), cost 7.901472
Iteration 750 (50 iterations in 43.93 seconds), cost 7.763008
Iteration 800 (50 iterations in 44.29 seconds), cost 7.631246
Iteration 850 (50 iterations in 44.62 seconds), cost 7.531365
Iteration 900 (50 iterations in 44.61 seconds), cost 7.399058
Iteration 950 (50 iterations in 44.91 seconds), cost 7.337746
Iteration 1000 (50 iterations in 44.45 seconds), cost 7.243399
Wrote the 1000000 x 2 data matrix successfully.
Done.

FIt-SNE: 1227.5842492580414
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000000 x 50 data matrix successfully. X[0,0] = 7.173379
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000000, K: 90, N*K = 90000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 166.354s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000147)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 43.48 seconds), cost 9.816879
Iteration 100 (50 iterations in 42.26 seconds), cost 9.827009
Iteration 150 (50 iterations in 41.57 seconds), cost 9.830330
Iteration 200 (50 iterations in 41.30 seconds), cost 9.798700
Iteration 250 (50 iterations in 41.47 seconds), cost 9.770814
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 42.06 seconds), cost 9.825242
Iteration 350 (50 iterations in 41.22 seconds), cost 9.842155
Iteration 400 (50 iterations in 41.45 seconds), cost 9.859394
Iteration 450 (50 iterations in 41.33 seconds), cost 9.830845
Iteration 500 (50 iterations in 43.86 seconds), cost 9.245599
Iteration 550 (50 iterations in 44.70 seconds), cost 8.651678
Iteration 600 (50 iterations in 44.13 seconds), cost 8.306504
Iteration 650 (50 iterations in 45.10 seconds), cost 8.086756
Iteration 700 (50 iterations in 44.85 seconds), cost 7.909652
Iteration 750 (50 iterations in 45.36 seconds), cost 7.770950
Iteration 800 (50 iterations in 45.72 seconds), cost 7.638463
Iteration 850 (50 iterations in 45.41 seconds), cost 7.529944
Iteration 900 (50 iterations in 44.78 seconds), cost 7.423620
Iteration 950 (50 iterations in 45.25 seconds), cost 7.317248
Iteration 1000 (50 iterations in 45.25 seconds), cost 7.244999
Wrote the 1000000 x 2 data matrix successfully.
Done.

FIt-SNE: 1241.0811231136322
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000000 x 50 data matrix successfully. X[0,0] = -0.966493
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000000, K: 90, N*K = 90000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 159.695s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000147)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 43.42 seconds), cost 9.796404
Iteration 100 (50 iterations in 42.68 seconds), cost 9.798616
Iteration 150 (50 iterations in 41.97 seconds), cost 9.814647
Iteration 200 (50 iterations in 41.71 seconds), cost 9.771707
Iteration 250 (50 iterations in 41.81 seconds), cost 9.780275
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 41.97 seconds), cost 9.821227
Iteration 350 (50 iterations in 41.99 seconds), cost 9.854993
Iteration 400 (50 iterations in 41.60 seconds), cost 9.853986
Iteration 450 (50 iterations in 41.75 seconds), cost 9.824309
Iteration 500 (50 iterations in 43.16 seconds), cost 9.238282
Iteration 550 (50 iterations in 44.89 seconds), cost 8.640765
Iteration 600 (50 iterations in 44.42 seconds), cost 8.328150
Iteration 650 (50 iterations in 44.97 seconds), cost 8.094805
Iteration 700 (50 iterations in 44.33 seconds), cost 7.903808
Iteration 750 (50 iterations in 44.47 seconds), cost 7.755174
Iteration 800 (50 iterations in 44.31 seconds), cost 7.615459
Iteration 850 (50 iterations in 44.75 seconds), cost 7.500263
Iteration 900 (50 iterations in 44.98 seconds), cost 7.416814
Iteration 950 (50 iterations in 43.92 seconds), cost 7.320763
Iteration 1000 (50 iterations in 44.07 seconds), cost 7.267078
Wrote the 1000000 x 2 data matrix successfully.
Done.

FIt-SNE: 1233.6159527301788
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000000 x 50 data matrix successfully. X[0,0] = 2.368051
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000000, K: 90, N*K = 90000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 153.146s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000147)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 43.51 seconds), cost 9.831856
Iteration 100 (50 iterations in 42.50 seconds), cost 9.836035
Iteration 150 (50 iterations in 41.44 seconds), cost 9.824950
Iteration 200 (50 iterations in 41.67 seconds), cost 9.822132
Iteration 250 (50 iterations in 41.64 seconds), cost 9.827274
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 41.51 seconds), cost 9.843482
Iteration 350 (50 iterations in 42.26 seconds), cost 9.865659
Iteration 400 (50 iterations in 41.46 seconds), cost 9.866372
Iteration 450 (50 iterations in 41.34 seconds), cost 9.781225
Iteration 500 (50 iterations in 43.69 seconds), cost 9.272189
Iteration 550 (50 iterations in 44.50 seconds), cost 8.680875
Iteration 600 (50 iterations in 44.49 seconds), cost 8.317576
Iteration 650 (50 iterations in 44.21 seconds), cost 8.117738
Iteration 700 (50 iterations in 44.79 seconds), cost 7.952443
Iteration 750 (50 iterations in 44.67 seconds), cost 7.801531
Iteration 800 (50 iterations in 45.02 seconds), cost 7.671281
Iteration 850 (50 iterations in 44.53 seconds), cost 7.553009
Iteration 900 (50 iterations in 44.90 seconds), cost 7.467013
Iteration 950 (50 iterations in 44.90 seconds), cost 7.378706
Iteration 1000 (50 iterations in 45.34 seconds), cost 7.293861
Wrote the 1000000 x 2 data matrix successfully.
Done.

FIt-SNE: 1227.675789117813
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 1000000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 1000000 x 50 data matrix successfully. X[0,0] = -1.909373
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 1000000, K: 90, N*K = 90000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 160.648s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000147)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 42.59 seconds), cost 9.786770
Iteration 100 (50 iterations in 42.21 seconds), cost 9.850100
Iteration 150 (50 iterations in 42.25 seconds), cost 9.797331
Iteration 200 (50 iterations in 41.99 seconds), cost 9.816837
Iteration 250 (50 iterations in 41.36 seconds), cost 9.815707
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 41.56 seconds), cost 9.865627
Iteration 350 (50 iterations in 40.87 seconds), cost 9.867452
Iteration 400 (50 iterations in 40.81 seconds), cost 9.863821
Iteration 450 (50 iterations in 41.30 seconds), cost 9.800704
Iteration 500 (50 iterations in 43.47 seconds), cost 9.292334
Iteration 550 (50 iterations in 43.72 seconds), cost 8.698712
Iteration 600 (50 iterations in 45.34 seconds), cost 8.341785
Iteration 650 (50 iterations in 44.73 seconds), cost 8.083906
Iteration 700 (50 iterations in 45.00 seconds), cost 7.902064
Iteration 750 (50 iterations in 45.08 seconds), cost 7.751349
Iteration 800 (50 iterations in 45.15 seconds), cost 7.592968
Iteration 850 (50 iterations in 45.05 seconds), cost 7.505060
Iteration 900 (50 iterations in 44.85 seconds), cost 7.408751
Iteration 950 (50 iterations in 45.09 seconds), cost 7.295381
Iteration 1000 (50 iterations in 45.49 seconds), cost 7.186123
Wrote the 1000000 x 2 data matrix successfully.
Done.

FIt-SNE: 1234.8369946479797
