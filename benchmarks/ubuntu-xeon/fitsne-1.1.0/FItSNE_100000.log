--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 100000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 100000 x 50 data matrix successfully. X[0,0] = -2.710379
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 100000, K: 90, N*K = 9000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 69.021s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.001413)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 11.80 seconds), cost 7.768828
Iteration 100 (50 iterations in 9.71 seconds), cost 7.768828
Iteration 150 (50 iterations in 9.66 seconds), cost 7.768824
Iteration 200 (50 iterations in 9.66 seconds), cost 7.721200
Iteration 250 (50 iterations in 9.70 seconds), cost 6.881090
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 9.75 seconds), cost 5.613685
Iteration 350 (50 iterations in 9.79 seconds), cost 5.184127
Iteration 400 (50 iterations in 9.73 seconds), cost 4.931452
Iteration 450 (50 iterations in 9.74 seconds), cost 4.752673
Iteration 500 (50 iterations in 9.74 seconds), cost 4.613586
Iteration 550 (50 iterations in 9.74 seconds), cost 4.499843
Iteration 600 (50 iterations in 9.74 seconds), cost 4.404826
Iteration 650 (50 iterations in 9.75 seconds), cost 4.324036
Iteration 700 (50 iterations in 9.75 seconds), cost 4.253744
Iteration 750 (50 iterations in 9.80 seconds), cost 4.191782
Iteration 800 (50 iterations in 9.75 seconds), cost 4.136369
Iteration 850 (50 iterations in 9.98 seconds), cost 4.086421
Iteration 900 (50 iterations in 9.98 seconds), cost 4.041236
Iteration 950 (50 iterations in 9.66 seconds), cost 4.000092
Iteration 1000 (50 iterations in 10.02 seconds), cost 3.962210
Wrote the 100000 x 2 data matrix successfully.
Done.

FIt-SNE: 280.3424906730652
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 100000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 100000 x 50 data matrix successfully. X[0,0] = -2.225409
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 100000, K: 90, N*K = 9000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 67.966s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.001414)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 10.00 seconds), cost 7.768927
Iteration 100 (50 iterations in 9.66 seconds), cost 7.768927
Iteration 150 (50 iterations in 9.60 seconds), cost 7.768921
Iteration 200 (50 iterations in 9.64 seconds), cost 7.693352
Iteration 250 (50 iterations in 9.66 seconds), cost 6.792681
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 9.78 seconds), cost 5.625117
Iteration 350 (50 iterations in 9.72 seconds), cost 5.184683
Iteration 400 (50 iterations in 9.73 seconds), cost 4.927645
Iteration 450 (50 iterations in 9.73 seconds), cost 4.745095
Iteration 500 (50 iterations in 9.73 seconds), cost 4.604063
Iteration 550 (50 iterations in 9.79 seconds), cost 4.489299
Iteration 600 (50 iterations in 9.74 seconds), cost 4.393363
Iteration 650 (50 iterations in 9.74 seconds), cost 4.311447
Iteration 700 (50 iterations in 9.75 seconds), cost 4.240117
Iteration 750 (50 iterations in 9.75 seconds), cost 4.177310
Iteration 800 (50 iterations in 9.83 seconds), cost 4.121251
Iteration 850 (50 iterations in 10.02 seconds), cost 4.070880
Iteration 900 (50 iterations in 10.18 seconds), cost 4.025111
Iteration 950 (50 iterations in 10.25 seconds), cost 3.983384
Iteration 1000 (50 iterations in 10.45 seconds), cost 3.945161
Wrote the 100000 x 2 data matrix successfully.
Done.

FIt-SNE: 278.34364223480225
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 100000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 100000 x 50 data matrix successfully. X[0,0] = 6.902491
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 100000, K: 90, N*K = 9000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 68.015s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.001415)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 10.36 seconds), cost 7.769011
Iteration 100 (50 iterations in 9.66 seconds), cost 7.769011
Iteration 150 (50 iterations in 9.65 seconds), cost 7.769005
Iteration 200 (50 iterations in 9.65 seconds), cost 7.711495
Iteration 250 (50 iterations in 9.63 seconds), cost 6.905206
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 9.77 seconds), cost 5.688434
Iteration 350 (50 iterations in 9.73 seconds), cost 5.244842
Iteration 400 (50 iterations in 9.73 seconds), cost 4.984705
Iteration 450 (50 iterations in 9.73 seconds), cost 4.797141
Iteration 500 (50 iterations in 9.80 seconds), cost 4.651029
Iteration 550 (50 iterations in 9.74 seconds), cost 4.532467
Iteration 600 (50 iterations in 9.74 seconds), cost 4.433774
Iteration 650 (50 iterations in 9.75 seconds), cost 4.349715
Iteration 700 (50 iterations in 9.75 seconds), cost 4.276639
Iteration 750 (50 iterations in 9.81 seconds), cost 4.212478
Iteration 800 (50 iterations in 9.75 seconds), cost 4.155582
Iteration 850 (50 iterations in 9.92 seconds), cost 4.104314
Iteration 900 (50 iterations in 10.04 seconds), cost 4.058073
Iteration 950 (50 iterations in 9.67 seconds), cost 4.015757
Iteration 1000 (50 iterations in 9.70 seconds), cost 3.976990
Wrote the 100000 x 2 data matrix successfully.
Done.

FIt-SNE: 277.33235573768616
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 100000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 100000 x 50 data matrix successfully. X[0,0] = -1.336489
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 100000, K: 90, N*K = 9000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 68.644s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.001415)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 10.56 seconds), cost 7.766814
Iteration 100 (50 iterations in 10.42 seconds), cost 7.766814
Iteration 150 (50 iterations in 10.82 seconds), cost 7.766807
Iteration 200 (50 iterations in 10.02 seconds), cost 7.688791
Iteration 250 (50 iterations in 9.78 seconds), cost 6.840606
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 9.76 seconds), cost 5.633138
Iteration 350 (50 iterations in 9.81 seconds), cost 5.201335
Iteration 400 (50 iterations in 9.76 seconds), cost 4.947032
Iteration 450 (50 iterations in 9.75 seconds), cost 4.765312
Iteration 500 (50 iterations in 9.81 seconds), cost 4.624261
Iteration 550 (50 iterations in 9.77 seconds), cost 4.508832
Iteration 600 (50 iterations in 9.77 seconds), cost 4.411922
Iteration 650 (50 iterations in 9.77 seconds), cost 4.329426
Iteration 700 (50 iterations in 9.82 seconds), cost 4.257928
Iteration 750 (50 iterations in 9.77 seconds), cost 4.194921
Iteration 800 (50 iterations in 9.77 seconds), cost 4.138787
Iteration 850 (50 iterations in 9.51 seconds), cost 4.088088
Iteration 900 (50 iterations in 9.81 seconds), cost 4.042437
Iteration 950 (50 iterations in 10.24 seconds), cost 4.000856
Iteration 1000 (50 iterations in 10.44 seconds), cost 3.962445
Wrote the 100000 x 2 data matrix successfully.
Done.

FIt-SNE: 281.5200819969177
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 1
Read the following parameters:
	 n 100000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 100000 x 50 data matrix successfully. X[0,0] = 6.124423
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 100000, K: 90, N*K = 9000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (1 threads):
[============================================================] 100% 68.089s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.001414)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 10.49 seconds), cost 7.769329
Iteration 100 (50 iterations in 10.42 seconds), cost 7.769329
Iteration 150 (50 iterations in 10.25 seconds), cost 7.769325
Iteration 200 (50 iterations in 9.71 seconds), cost 7.712101
Iteration 250 (50 iterations in 9.70 seconds), cost 6.809579
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 9.79 seconds), cost 5.623969
Iteration 350 (50 iterations in 9.73 seconds), cost 5.197229
Iteration 400 (50 iterations in 9.74 seconds), cost 4.947641
Iteration 450 (50 iterations in 9.74 seconds), cost 4.769920
Iteration 500 (50 iterations in 9.80 seconds), cost 4.631638
Iteration 550 (50 iterations in 9.74 seconds), cost 4.518140
Iteration 600 (50 iterations in 9.75 seconds), cost 4.422372
Iteration 650 (50 iterations in 9.76 seconds), cost 4.340500
Iteration 700 (50 iterations in 9.75 seconds), cost 4.269334
Iteration 750 (50 iterations in 9.76 seconds), cost 4.206624
Iteration 800 (50 iterations in 9.80 seconds), cost 4.150779
Iteration 850 (50 iterations in 10.04 seconds), cost 4.100452
Iteration 900 (50 iterations in 10.18 seconds), cost 4.054593
Iteration 950 (50 iterations in 10.24 seconds), cost 4.012722
Iteration 1000 (50 iterations in 10.37 seconds), cost 3.973957
Wrote the 100000 x 2 data matrix successfully.
Done.

FIt-SNE: 280.5377984046936
