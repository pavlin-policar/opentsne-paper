--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 250000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 250000 x 50 data matrix successfully. X[0,0] = -1.792886
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 250000, K: 90, N*K = 22500000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 29.365s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000576)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 13.00 seconds), cost 8.429484
Iteration 100 (50 iterations in 12.88 seconds), cost 8.492549
Iteration 150 (50 iterations in 12.94 seconds), cost 8.467911
Iteration 200 (50 iterations in 12.76 seconds), cost 8.514796
Iteration 250 (50 iterations in 12.93 seconds), cost 8.540382
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 12.97 seconds), cost 8.250006
Iteration 350 (50 iterations in 13.00 seconds), cost 6.995445
Iteration 400 (50 iterations in 13.20 seconds), cost 6.484014
Iteration 450 (50 iterations in 13.14 seconds), cost 6.154927
Iteration 500 (50 iterations in 13.08 seconds), cost 6.000086
Iteration 550 (50 iterations in 13.14 seconds), cost 5.827840
Iteration 600 (50 iterations in 13.25 seconds), cost 5.695463
Iteration 650 (50 iterations in 13.14 seconds), cost 5.579628
Iteration 700 (50 iterations in 13.13 seconds), cost 5.470607
Iteration 750 (50 iterations in 13.17 seconds), cost 5.400501
Iteration 800 (50 iterations in 13.37 seconds), cost 5.328520
Iteration 850 (50 iterations in 13.28 seconds), cost 5.255039
Iteration 900 (50 iterations in 13.15 seconds), cost 5.158005
Iteration 950 (50 iterations in 13.33 seconds), cost 5.096976
Iteration 1000 (50 iterations in 13.12 seconds), cost 5.095427
Wrote the 250000 x 2 data matrix successfully.
Done.

FIt-SNE: 332.22956919670105
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 250000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 250000 x 50 data matrix successfully. X[0,0] = -2.640799
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 250000, K: 90, N*K = 22500000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 30.283s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000576)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 12.59 seconds), cost 8.445090
Iteration 100 (50 iterations in 12.59 seconds), cost 8.520756
Iteration 150 (50 iterations in 12.92 seconds), cost 8.531863
Iteration 200 (50 iterations in 12.67 seconds), cost 8.521743
Iteration 250 (50 iterations in 12.37 seconds), cost 8.520043
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 12.18 seconds), cost 8.136385
Iteration 350 (50 iterations in 12.93 seconds), cost 6.976001
Iteration 400 (50 iterations in 13.09 seconds), cost 6.546574
Iteration 450 (50 iterations in 13.35 seconds), cost 6.234777
Iteration 500 (50 iterations in 13.21 seconds), cost 6.027119
Iteration 550 (50 iterations in 13.12 seconds), cost 5.849474
Iteration 600 (50 iterations in 13.23 seconds), cost 5.751876
Iteration 650 (50 iterations in 13.06 seconds), cost 5.641807
Iteration 700 (50 iterations in 13.45 seconds), cost 5.518716
Iteration 750 (50 iterations in 13.23 seconds), cost 5.449658
Iteration 800 (50 iterations in 13.16 seconds), cost 5.376616
Iteration 850 (50 iterations in 13.18 seconds), cost 5.278399
Iteration 900 (50 iterations in 13.18 seconds), cost 5.212027
Iteration 950 (50 iterations in 13.16 seconds), cost 5.185408
Iteration 1000 (50 iterations in 13.09 seconds), cost 5.100970
Wrote the 250000 x 2 data matrix successfully.
Done.

FIt-SNE: 331.10151839256287
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 250000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 250000 x 50 data matrix successfully. X[0,0] = 53.140439
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 250000, K: 90, N*K = 22500000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 29.778s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000576)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 12.95 seconds), cost 8.461030
Iteration 100 (50 iterations in 12.70 seconds), cost 8.523066
Iteration 150 (50 iterations in 12.88 seconds), cost 8.525212
Iteration 200 (50 iterations in 12.63 seconds), cost 8.439238
Iteration 250 (50 iterations in 12.83 seconds), cost 8.437423
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 12.90 seconds), cost 8.296640
Iteration 350 (50 iterations in 12.70 seconds), cost 7.051966
Iteration 400 (50 iterations in 13.10 seconds), cost 6.597763
Iteration 450 (50 iterations in 13.27 seconds), cost 6.283984
Iteration 500 (50 iterations in 13.19 seconds), cost 6.076597
Iteration 550 (50 iterations in 12.88 seconds), cost 5.903072
Iteration 600 (50 iterations in 13.05 seconds), cost 5.768559
Iteration 650 (50 iterations in 13.39 seconds), cost 5.599629
Iteration 700 (50 iterations in 13.39 seconds), cost 5.552539
Iteration 750 (50 iterations in 13.19 seconds), cost 5.463052
Iteration 800 (50 iterations in 13.20 seconds), cost 5.385591
Iteration 850 (50 iterations in 13.29 seconds), cost 5.316367
Iteration 900 (50 iterations in 13.20 seconds), cost 5.206775
Iteration 950 (50 iterations in 13.39 seconds), cost 5.154156
Iteration 1000 (50 iterations in 12.86 seconds), cost 5.132783
Wrote the 250000 x 2 data matrix successfully.
Done.

FIt-SNE: 331.5903916358948
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 250000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 250000 x 50 data matrix successfully. X[0,0] = -2.179824
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 250000, K: 90, N*K = 22500000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 29.805s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000576)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 12.62 seconds), cost 8.461624
Iteration 100 (50 iterations in 12.22 seconds), cost 8.520526
Iteration 150 (50 iterations in 12.70 seconds), cost 8.452692
Iteration 200 (50 iterations in 12.30 seconds), cost 8.510791
Iteration 250 (50 iterations in 12.36 seconds), cost 8.466920
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 12.52 seconds), cost 8.122222
Iteration 350 (50 iterations in 12.51 seconds), cost 6.947566
Iteration 400 (50 iterations in 12.78 seconds), cost 6.548705
Iteration 450 (50 iterations in 12.60 seconds), cost 6.250643
Iteration 500 (50 iterations in 12.73 seconds), cost 6.030344
Iteration 550 (50 iterations in 12.93 seconds), cost 5.912378
Iteration 600 (50 iterations in 12.98 seconds), cost 5.747348
Iteration 650 (50 iterations in 12.88 seconds), cost 5.634303
Iteration 700 (50 iterations in 12.95 seconds), cost 5.577903
Iteration 750 (50 iterations in 12.80 seconds), cost 5.475626
Iteration 800 (50 iterations in 12.71 seconds), cost 5.373022
Iteration 850 (50 iterations in 12.85 seconds), cost 5.325492
Iteration 900 (50 iterations in 12.99 seconds), cost 5.270177
Iteration 950 (50 iterations in 12.77 seconds), cost 5.202738
Iteration 1000 (50 iterations in 12.96 seconds), cost 5.143982
Wrote the 250000 x 2 data matrix successfully.
Done.

FIt-SNE: 325.1543354988098
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 250000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 250000 x 50 data matrix successfully. X[0,0] = 3.305749
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 250000, K: 90, N*K = 22500000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 29.42s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000576)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 13.05 seconds), cost 8.451201
Iteration 100 (50 iterations in 12.88 seconds), cost 8.519805
Iteration 150 (50 iterations in 12.95 seconds), cost 8.522097
Iteration 200 (50 iterations in 12.84 seconds), cost 8.476910
Iteration 250 (50 iterations in 12.87 seconds), cost 8.520582
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 12.69 seconds), cost 8.224496
Iteration 350 (50 iterations in 13.37 seconds), cost 7.061711
Iteration 400 (50 iterations in 13.40 seconds), cost 6.572368
Iteration 450 (50 iterations in 13.16 seconds), cost 6.274845
Iteration 500 (50 iterations in 13.18 seconds), cost 6.069550
Iteration 550 (50 iterations in 13.23 seconds), cost 5.864468
Iteration 600 (50 iterations in 13.27 seconds), cost 5.695777
Iteration 650 (50 iterations in 12.88 seconds), cost 5.604386
Iteration 700 (50 iterations in 13.51 seconds), cost 5.523971
Iteration 750 (50 iterations in 13.25 seconds), cost 5.393236
Iteration 800 (50 iterations in 13.27 seconds), cost 5.355160
Iteration 850 (50 iterations in 13.22 seconds), cost 5.284329
Iteration 900 (50 iterations in 13.12 seconds), cost 5.198342
Iteration 950 (50 iterations in 12.99 seconds), cost 5.161407
Iteration 1000 (50 iterations in 13.44 seconds), cost 5.067480
Wrote the 250000 x 2 data matrix successfully.
Done.

FIt-SNE: 332.7948958873749
