--------------------------------------------------------------------------------
Random state 0
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 500000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 500000 x 50 data matrix successfully. X[0,0] = -3.462344
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 500000, K: 90, N*K = 45000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 68.177s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000291)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 22.85 seconds), cost 9.094927
Iteration 100 (50 iterations in 21.78 seconds), cost 9.150980
Iteration 150 (50 iterations in 21.96 seconds), cost 9.184236
Iteration 200 (50 iterations in 22.08 seconds), cost 9.169518
Iteration 250 (50 iterations in 21.43 seconds), cost 9.105224
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 22.14 seconds), cost 9.185094
Iteration 350 (50 iterations in 21.88 seconds), cost 9.166571
Iteration 400 (50 iterations in 23.22 seconds), cost 8.240320
Iteration 450 (50 iterations in 23.75 seconds), cost 7.579450
Iteration 500 (50 iterations in 24.00 seconds), cost 7.223402
Iteration 550 (50 iterations in 24.09 seconds), cost 7.025106
Iteration 600 (50 iterations in 24.05 seconds), cost 6.824490
Iteration 650 (50 iterations in 23.87 seconds), cost 6.693535
Iteration 700 (50 iterations in 24.08 seconds), cost 6.539229
Iteration 750 (50 iterations in 23.90 seconds), cost 6.433785
Iteration 800 (50 iterations in 23.94 seconds), cost 6.364200
Iteration 850 (50 iterations in 23.89 seconds), cost 6.253846
Iteration 900 (50 iterations in 24.00 seconds), cost 6.173924
Iteration 950 (50 iterations in 23.62 seconds), cost 6.130778
Iteration 1000 (50 iterations in 24.39 seconds), cost 6.048099
Wrote the 500000 x 2 data matrix successfully.
Done.

FIt-SNE: 624.9882161617279
--------------------------------------------------------------------------------
Random state 1
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 500000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 500000 x 50 data matrix successfully. X[0,0] = 0.581272
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 500000, K: 90, N*K = 45000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 67.512s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000291)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 23.73 seconds), cost 9.069046
Iteration 100 (50 iterations in 22.56 seconds), cost 9.153221
Iteration 150 (50 iterations in 22.08 seconds), cost 9.078871
Iteration 200 (50 iterations in 21.99 seconds), cost 9.148992
Iteration 250 (50 iterations in 21.65 seconds), cost 9.161465
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 22.45 seconds), cost 9.189382
Iteration 350 (50 iterations in 21.84 seconds), cost 9.142761
Iteration 400 (50 iterations in 23.17 seconds), cost 8.359978
Iteration 450 (50 iterations in 23.95 seconds), cost 7.692195
Iteration 500 (50 iterations in 23.98 seconds), cost 7.309953
Iteration 550 (50 iterations in 24.03 seconds), cost 7.124467
Iteration 600 (50 iterations in 24.07 seconds), cost 6.945417
Iteration 650 (50 iterations in 24.11 seconds), cost 6.790402
Iteration 700 (50 iterations in 23.79 seconds), cost 6.668890
Iteration 750 (50 iterations in 24.01 seconds), cost 6.560772
Iteration 800 (50 iterations in 24.15 seconds), cost 6.459206
Iteration 850 (50 iterations in 23.68 seconds), cost 6.347687
Iteration 900 (50 iterations in 24.13 seconds), cost 6.254215
Iteration 950 (50 iterations in 23.75 seconds), cost 6.224377
Iteration 1000 (50 iterations in 24.14 seconds), cost 6.126441
Wrote the 500000 x 2 data matrix successfully.
Done.

FIt-SNE: 628.7128388881683
--------------------------------------------------------------------------------
Random state 2
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 500000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 500000 x 50 data matrix successfully. X[0,0] = -2.508996
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 500000, K: 90, N*K = 45000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 69.79s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000291)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 23.15 seconds), cost 9.112936
Iteration 100 (50 iterations in 22.04 seconds), cost 9.150984
Iteration 150 (50 iterations in 22.08 seconds), cost 9.151999
Iteration 200 (50 iterations in 22.12 seconds), cost 9.121270
Iteration 250 (50 iterations in 21.98 seconds), cost 9.098883
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 21.83 seconds), cost 9.180834
Iteration 350 (50 iterations in 21.66 seconds), cost 9.131141
Iteration 400 (50 iterations in 23.07 seconds), cost 8.240924
Iteration 450 (50 iterations in 24.24 seconds), cost 7.624917
Iteration 500 (50 iterations in 24.54 seconds), cost 7.300491
Iteration 550 (50 iterations in 24.65 seconds), cost 7.040555
Iteration 600 (50 iterations in 24.18 seconds), cost 6.869403
Iteration 650 (50 iterations in 24.41 seconds), cost 6.693955
Iteration 700 (50 iterations in 23.83 seconds), cost 6.593944
Iteration 750 (50 iterations in 24.14 seconds), cost 6.493164
Iteration 800 (50 iterations in 24.41 seconds), cost 6.396885
Iteration 850 (50 iterations in 24.13 seconds), cost 6.306885
Iteration 900 (50 iterations in 24.23 seconds), cost 6.204966
Iteration 950 (50 iterations in 24.20 seconds), cost 6.157641
Iteration 1000 (50 iterations in 23.95 seconds), cost 6.095909
Wrote the 500000 x 2 data matrix successfully.
Done.

FIt-SNE: 630.8328459262848
--------------------------------------------------------------------------------
Random state 3
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 500000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 500000 x 50 data matrix successfully. X[0,0] = -2.709226
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 500000, K: 90, N*K = 45000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 69.045s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000291)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 22.85 seconds), cost 9.110743
Iteration 100 (50 iterations in 22.21 seconds), cost 9.145200
Iteration 150 (50 iterations in 22.67 seconds), cost 9.138358
Iteration 200 (50 iterations in 22.03 seconds), cost 9.118029
Iteration 250 (50 iterations in 21.54 seconds), cost 9.150782
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 22.19 seconds), cost 9.184399
Iteration 350 (50 iterations in 21.56 seconds), cost 9.134676
Iteration 400 (50 iterations in 22.73 seconds), cost 8.284674
Iteration 450 (50 iterations in 23.50 seconds), cost 7.608822
Iteration 500 (50 iterations in 24.33 seconds), cost 7.309184
Iteration 550 (50 iterations in 24.15 seconds), cost 7.071506
Iteration 600 (50 iterations in 23.55 seconds), cost 6.865525
Iteration 650 (50 iterations in 23.85 seconds), cost 6.734784
Iteration 700 (50 iterations in 23.99 seconds), cost 6.631370
Iteration 750 (50 iterations in 23.86 seconds), cost 6.501847
Iteration 800 (50 iterations in 24.08 seconds), cost 6.426091
Iteration 850 (50 iterations in 23.94 seconds), cost 6.341242
Iteration 900 (50 iterations in 24.21 seconds), cost 6.270315
Iteration 950 (50 iterations in 23.92 seconds), cost 6.181312
Iteration 1000 (50 iterations in 23.95 seconds), cost 6.111752
Wrote the 500000 x 2 data matrix successfully.
Done.

FIt-SNE: 626.308650970459
--------------------------------------------------------------------------------
Random state 4
--------------------------------------------------------------------------------
=============== t-SNE v1.1.0 ===============
fast_tsne data_path: data.dat
fast_tsne result_path: result.dat
fast_tsne nthreads: 8
Read the following parameters:
	 n 500000 by d 50 dataset, theta 0.500000,
	 perplexity 30.000000, no_dims 2, max_iter 1000,
	 stop_lying_iter 250, mom_switch_iter 250,
	 momentum 0.500000, final_momentum 0.800000,
	 learning_rate 200.000000, K -1, sigma -1.000000, nbody_algo 2,
	 knn_algo 1, early_exag_coeff 12.000000,
	 no_momentum_during_exag 0, n_trees 50, search_k 4500,
	 start_late_exag_iter -1, late_exag_coeff -1.000000
	 nterms 3, interval_per_integer 1.000000, min_num_intervals 50, t-dist df 1.000000
Read the 500000 x 50 data matrix successfully. X[0,0] = -3.957976
Read the initialization successfully.
Will use momentum during exaggeration phase
Computing input similarities...
Using perplexity, so normalizing input data (to prevent numerical problems)
Using perplexity, not the manually set kernel width.  K (number of nearest neighbors) and sigma (bandwidth) parameters are going to be ignored.
Using ANNOY for knn search, with parameters: n_trees 50 and search_k 4500
Going to allocate memory. N: 500000, K: 90, N*K = 45000000
Building Annoy tree...
Done building tree. Beginning nearest neighbor search... 
parallel (8 threads):
[===========================================================>] 100% 69.325s
Symmetrizing...
Using the given initialization.
Exaggerating Ps by 12.000000
Input similarities computed (sparsity = 0.000291)!
Learning embedding...
Using FIt-SNE approximation.
Iteration 50 (50 iterations in 22.55 seconds), cost 9.198651
Iteration 100 (50 iterations in 22.32 seconds), cost 9.171271
Iteration 150 (50 iterations in 22.60 seconds), cost 9.181344
Iteration 200 (50 iterations in 22.06 seconds), cost 9.163654
Iteration 250 (50 iterations in 21.28 seconds), cost 9.167976
Unexaggerating Ps by 12.000000
Iteration 300 (50 iterations in 21.80 seconds), cost 9.161136
Iteration 350 (50 iterations in 22.07 seconds), cost 9.139614
Iteration 400 (50 iterations in 23.15 seconds), cost 8.247165
Iteration 450 (50 iterations in 23.31 seconds), cost 7.649424
Iteration 500 (50 iterations in 24.07 seconds), cost 7.309676
Iteration 550 (50 iterations in 24.02 seconds), cost 7.075661
Iteration 600 (50 iterations in 23.74 seconds), cost 6.882650
Iteration 650 (50 iterations in 23.55 seconds), cost 6.768266
Iteration 700 (50 iterations in 23.90 seconds), cost 6.585916
Iteration 750 (50 iterations in 23.52 seconds), cost 6.524202
Iteration 800 (50 iterations in 24.11 seconds), cost 6.432970
Iteration 850 (50 iterations in 24.12 seconds), cost 6.306042
Iteration 900 (50 iterations in 24.05 seconds), cost 6.263710
Iteration 950 (50 iterations in 24.13 seconds), cost 6.180723
Iteration 1000 (50 iterations in 23.98 seconds), cost 6.080875
Wrote the 500000 x 2 data matrix successfully.
Done.

FIt-SNE: 624.568351984024
