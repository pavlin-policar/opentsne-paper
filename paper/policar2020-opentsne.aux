\relax 
\global\@namedef{num@address}{3}
\global\@namedef{num@author}{3}
\citation{maaten2008visualizing}
\citation{2018arXivUMAP}
\citation{macosko2015highly}
\citation{cao2019single}
\citation{tasic2018shared}
\citation{hirata2019genetic}
\citation{beaulaurier2018metagenomic}
\citation{sheth2019spatial}
\citation{tkachev2019differences}
\citation{macosko2015highly}
\citation{shekhar2016comprehensive}
\citation{maaten2008visualizing}
\citation{ding2018interpretable}
\citation{becht2019dimensionality}
\citation{linderman2019fast}
\citation{kobak2019art}
\citation{policar2019embedding}
\citation{van2014accelerating}
\citation{linderman2019fast}
\citation{policar2019embedding}
\citation{kobak2019umap}
\citation{kobak2019heavy}
\citation{kobak2019art}
\citation{belkina2019automated}
\thanksnewlabel{au1@email}{{pavlin.policar@fri.uni-lj.si}{1}}
\thanksnewlabel{au2@email}{{martin.strazar@gmail.com}{1}}
\thanksnewlabel{au3@email}{{martin.strazar@gmail.com}{1}}
\@writefile{toc}{\contentsline {section}{Abstract}{1}\protected@file@percent }
\@LN@col{1}
\thanksnewlabel{bmc@corref@authorthanks}{{*}{1}}
\@LN@col{2}
\citation{sklearn_api}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  We use \textsf  {openTSNE}\ to generate three t-SNE embeddings and demonstrate recent theoretical advances. The data in \fig@textbf  {(a)} and \fig@textbf  {(b)} represent 44,808 single-cell gene-expression profiles of mouse retinal cells from Macosko \textit  {et al.}\nobreakspace  {}\cite  {macosko2015highly}. The data in \fig@textbf  {(c)} additionally contains 27,499 expression profiles from mouse retinal cells from Shekhar \textit  {et al.}\nobreakspace  {}\cite  {shekhar2016comprehensive}. \fig@textbf  {(a)} We construct a t-SNE embedding following the parameter choices from the original publication by Maaten \& Hinton\nobreakspace  {}\cite  {maaten2008visualizing}. The visualization shows no preservation of the global organization of clusters, resulting from random initialization and an affinity model focused on preserving local neighborhoods. \fig@textbf  {(b)} A modern t-SNE embedding, utilizing the latest theoretical advances and practical recommendations constructed using a multi-scale affinity model, preserving both short-range and long-range interactions between data points and initialized so that the global layout is as meaningful as possible. Unlike in \fig@textbf  {(a)}, the green and blue clusters representing different sub-types of amacrine and bipolar cells are now localized to the same regions of the space, indicating a higher level of similarity than to other cell types. The embedding in \fig@textbf  {(c)} shows how existing t-SNE reference atlases can be used to place new samples into existing embeddings. The positions of new data points correspond to cell types from the reference atlas. }}{2}\protected@file@percent }
\newlabel{fig:macosko}{{1}{2}}
\@LN@col{1}
\@LN@col{2}
\citation{tasic2018shared}
\citation{tasic2018shared}
\citation{kobak2019art}
\citation{cao2019single}
\citation{jacomy2014forceatlas2}
\citation{bohm2020unifying}
\citation{policar2019embedding}
\citation{macosko2015highly}
\citation{shekhar2016comprehensive}
\citation{hochgerner2018conserved}
\citation{harris2018classes}
\citation{hochgerner2018conserved}
\citation{harris2018classes}
\@LN@col{1}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  We use \textsf  {openTSNE}\ to create four different visualizations of the Tasic \textit  {et al.}\nobreakspace  {}\cite  {tasic2018shared} data, each providing a different perspective into the topology of the data. The data set contains 21,874 single-cells originating from the mouse neocortex. Cluster annotations and colors are taken from the original publication. Warm colors correspond to excitatory neurons, cool colors correspond to inhibitory neurons, and gray/brown colors correspond to non-neuronal cells. Standard t-SNE \fig@textbf  {(a)} emphasizes local structure while increasing perplexity \fig@textbf  {(b)} results in a more meaningful layout of the clusters. We can also combine the two perplexities by using a multiscale kernel affinity model \fig@textbf  {(c)} and obtain a trade-off between global and local structure. Alternatively, we can inspect more fine-grained structure and reveal smaller clusters by using a more heavy-tailed kernel \fig@textbf  {(d)}.}}{4}\protected@file@percent }
\newlabel{fig:tasic}{{2}{4}}
\citation{kobak2019art}
\citation{becht2019dimensionality}
\citation{van2014accelerating}
\citation{Ulyanov2016}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Increasing the exaggeration parameter leads to compact clusters, highlighting the data's global organization and emphasizing the continuous nature of cell state transitions. The data set from Cao \textit  {et al.}\nobreakspace  {}\cite  {cao2019single} contains expression profiles from 2,058,652 single cells. The data were collected from mice embryos at different developmental stages at daily intervals after 9.5 to 13.5 days. \fig@textbf  {(c)} reveals that the data is comprised of two main components -- the neural tube and mesenchymal cells -- as well as several other smaller clusters. The colors indicate developmental progression with red indicating least-developed cells and blue indicating most developed cells. The overall developmental trajectory is most apparent with higher exaggeration levels, showing red cells slowly transitioning into blue cells. Progressively easing the exaggeration factor uncovers finer clusters within the larger groups, as shown in \fig@textbf  {(b)} with exaggeration of two and subsequently in \fig@textbf  {(a)}, where we show the standard t-SNE with no exaggeration. 32,011 putative doublets are excluded from the visualizations. }}{5}\protected@file@percent }
\newlabel{fig:cao}{{3}{5}}
\@LN@col{1}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \textsf  {openTSNE}\ supports embedding new samples into an existing reference t-SNE landscape. For the series of visualizations shown in this figure, we first construct a t-SNE embedding for the data from Hochgerner \textit  {et al.}\nobreakspace  {}\cite  {hochgerner2018conserved} containing 24,185 developing, single cells from the mouse hippocampus. The data contains gene expression in different neurons, supporting glia, and other vascular cells (upper left). Data points representing cells are colored according to cell-types assigned in the original publication; see the legend from Fig.\nobreakspace  {}2\hbox {} to map colors to cell-type. We then embed new, hippocampal cells collected in a study by Harris \textit  {et al.}\nobreakspace  {}\cite  {harris2018classes} using the embedding of Hochgerner \textit  {et al.} data as a reference. In their study, Harris \textit  {et al.} collected 6,971 single-cells and focused on identifying different types of inhibitory neurons. However, almost half of the collected cells are not neurons and were left uncharacterized. Inspecting the embeddings of these cells in the reference embedding (bottom left) reveals that in addition to inhibitory neurons, the data contains several supporting glial cells as well as a small population of endothelial cells. We can verify our approach's accuracy by inspecting marker genes for the major cell types in the reference (top row) and embedded samples (bottom row). }}{6}\protected@file@percent }
\newlabel{fig:transform}{{4}{6}}
\citation{pedregosa2011scikit}
\citation{Ulyanov2016}
\citation{linderman2019fast}
\citation{wong2012points}
\@LN@col{1}
\@LN@col{2}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Features of \textsf  {openTSNE}\ compared to other three popular open-source implementations from \textsf  {scikit-learn}\nobreakspace  {}(v0.23.1), \textsf  {MulticoreTSNE} (v0.1), and \textsf  {FIt-SNE} (v1.1.0). The first section of the comparison addresses packaging and distribution. A properly packaged library is easily accessible to users, and developers should easily include it in dependency lists of other software packages. The second section of the comparison lists the two existing t-SNE approximation schemes. The FIt-SNE approximation scheme is required for t-SNE to scale up to millions of data points. The final section provides a list of extensions and improvements to the standard t-SNE algorithm, many of which can produce markedly better visualizations. }}{7}\protected@file@percent }
\newlabel{tab:features}{{1}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  We benchmark \textsf  {openTSNE}\ (v0.4.3) against three popular open-source implementations from \textsf  {scikit-learn}\nobreakspace  {}\cite  {pedregosa2011scikit} (v0.23.1), \textsf  {MulticoreTSNE}\nobreakspace  {}\cite  {Ulyanov2016} (v0.1), and \textsf  {FIt-SNE}\nobreakspace  {}\cite  {linderman2019fast} (v1.1.0). Experiments were run on a consumer-grade Intel Core i7-7700HQ processor found in laptop computers, and on a server-grade Intel Xeon E5-2650. To generate benchmark data sets of different sizes, we subsampled data from the 10X Genomics 1.3 million mouse brain data set five times, resulting in five different data sets for each size. In total, we run each implementation on 30 different data sets. Notice that \textsf  {openTSNE}\ scales similarly to \textsf  {FIt-SNE}, as they both use the same interpolation-based approximation scheme, while \textsf  {scikit-learn} and \textsf  {MulticoreTSNE} utilize the Barnes-Hut approximation. }}{8}\protected@file@percent }
\newlabel{fig:benchmarks}{{5}{8}}
\@LN@col{1}
\@LN@col{2}
\newlabel{eq:gaussian_kernel}{{1}{8}}
\newlabel{eq:symmetrize}{{2}{8}}
\citation{jacobs1988increased}
\citation{belkina2019automated}
\citation{van2014accelerating}
\citation{van2014accelerating}
\citation{linderman2019fast}
\citation{van2014accelerating}
\@LN@col{1}
\newlabel{eq:cauchy_kernel}{{5}{9}}
\newlabel{eq:kl_divergence}{{6}{9}}
\newlabel{eq:tsne_gradient}{{7}{9}}
\@LN@col{2}
\newlabel{eq:grad_attr_rep}{{8}{9}}
\citation{linderman2019fast}
\citation{policar2019embedding}
\citation{kobak2019art}
\@LN@col{1}
\newlabel{eq:fitsne_rep}{{9}{10}}
\newlabel{eq:fitsne_terms}{{10}{10}}
\@LN@col{2}
\newlabel{eq:gradient}{{13}{10}}
\citation{kobak2019heavy}
\citation{2018arXivUMAP}
\citation{jacomy2014forceatlas2}
\citation{bohm2020unifying}
\@LN@col{1}
\@LN@col{2}
\bibstyle{bmc-mathphys}
\bibdata{references}
\bibcite{maaten2008visualizing}{1}
\bibcite{2018arXivUMAP}{2}
\bibcite{macosko2015highly}{3}
\bibcite{cao2019single}{4}
\bibcite{tasic2018shared}{5}
\bibcite{hirata2019genetic}{6}
\bibcite{beaulaurier2018metagenomic}{7}
\bibcite{sheth2019spatial}{8}
\bibcite{tkachev2019differences}{9}
\bibcite{shekhar2016comprehensive}{10}
\bibcite{ding2018interpretable}{11}
\bibcite{becht2019dimensionality}{12}
\bibcite{linderman2019fast}{13}
\bibcite{kobak2019art}{14}
\bibcite{policar2019embedding}{15}
\bibcite{van2014accelerating}{16}
\bibcite{kobak2019umap}{17}
\bibcite{kobak2019heavy}{18}
\bibcite{belkina2019automated}{19}
\bibcite{sklearn_api}{20}
\bibcite{jacomy2014forceatlas2}{21}
\bibcite{bohm2020unifying}{22}
\@LN@col{1}
\thanksnewlabel{aff1thanks}{{1}{12}}
\thanksnewlabel{aff2thanks}{{2}{12}}
\thanksnewlabel{aff3thanks}{{3}{12}}
\@LN@col{2}
\bibcite{hochgerner2018conserved}{23}
\bibcite{harris2018classes}{24}
\bibcite{Ulyanov2016}{25}
\bibcite{pedregosa2011scikit}{26}
\bibcite{wong2012points}{27}
\bibcite{jacobs1988increased}{28}
\@LN@col{1}
\@LN@col{2}
\newlabel{LastPage}{{}{13}}
\xdef\lastpage@lastpage{13}
\gdef\lastpage@lastpageHy{}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  vs.pygstyle,
  FEB2C09EB0D42584785DCC6D545A7029642B7C798BB699DDC77AD5B0663F36A4.pygtex}
